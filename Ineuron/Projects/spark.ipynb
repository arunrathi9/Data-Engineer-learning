{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Project\n",
    "\n",
    "**Dataset** - https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset\n",
    "\n",
    "Step 1 - Create a producer with a python connector in confluent kafka and\n",
    "stream your data.\n",
    "\n",
    "Step 2 - Consume your data through the python connector and dump it in\n",
    "mongodb atlas.\n",
    "Note: Here in the dataset you will be finding a multiple files you\n",
    "need to use all file for the kafka and mongodb\n",
    "\n",
    "Step 3 - Collect your data as a pyspark dataframe and perform different\n",
    "operations.<br>\n",
    "Note: Consider only three files for creating a dataframe among all\n",
    "case, region and TimeProvince<br>\n",
    "1. Read the data, show it and Count the number of records.\n",
    "2. Describe the data with a describe function.\n",
    "3. If there is any duplicate value drop it.\n",
    "4. Use limit function for showcasing a limited number of\n",
    "records.\n",
    "5. If you find the column name is not suitable, change the\n",
    "column name.[optional]\n",
    "6. Select the subset of the columns.\n",
    "7. If there is any null value, fill it with any random value or drop\n",
    "it.\n",
    "8. Filter the data based on different columns or variables and\n",
    "do the best analysis.\n",
    "<br>For example: We can filter a data frame using multiple\n",
    "conditions using AND(&), OR(|) and NOT(~) conditions. For\n",
    "example, we may want to find out all the dif erent\n",
    "infection_case in Daegu Province with more than 10\n",
    "confirmed cases.</br>\n",
    "9. Sort the number of confirmed cases. Confirmed column is\n",
    "there in the dataset. Check with descending sort also.\n",
    "10. In case of any wrong data type, cast that data type from\n",
    "integer to string or string to integer.\n",
    "Use group by on top of province and city column and agg it\n",
    "with sum of confirmed cases. For example\n",
    "df.groupBy([\"province\",\"city\"]).agg(function.sum(\"co\n",
    "nfirmed\")\n",
    "11. For joins we will need one more file.you can use region file.\n",
    "User different different join methods.for example\n",
    "cases.join(regions, ['province','city'],how='left')\n",
    "You can do your best analysis.\n",
    "\n",
    "Step 5 - If you want, you can also use SQL with data frames. Let us try to\n",
    "run some SQL on the cases table.<br>\n",
    "For example:<br>\n",
    "cases.registerTempTable('cases_table')<br>\n",
    "newDF = sqlContext.sql('select * from cases_table where\n",
    "confirmed>100')<br>\n",
    "newDF.show()\n",
    "<br>\n",
    "<t>\n",
    "<br>\n",
    "Here is a example how you can use df for sql now you can perform\n",
    "various operations with GROUP BY, HAVING, AND ORDER BY\n",
    "\n",
    "Step 6 - Create Spark UDFs\n",
    "Create function casehighlow()<br>\n",
    "If case is less than 50 return low else return high<br>\n",
    "convert into a UDF Function and mention the return type of\n",
    "function.<br>\n",
    "Note: You can create as many as udf based on analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle datasets download -d kimjihoo/coronavirusdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
