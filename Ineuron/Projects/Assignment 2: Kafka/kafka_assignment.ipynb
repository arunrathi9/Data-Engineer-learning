{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download Data Link** -> https://github.com/shashank-mishra219/Confluent-Kafka-Setup/blob/main/restaurant_orders.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Confluent Kafka Account"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create one kafka topic named as \"restaurent-take-away-data\" with 3 partitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup key (string) & value (json) schema in the confluent schema registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import build_table_schema\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_basic_schema = {\n",
    "  \"$id\": \"http://example.com/myURI.schema.json\",\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"additionalProperties\": False,\n",
    "  \"description\": \"Sample schema to help you get started.\",\n",
    "  \"title\": \"SampleRecord\",\n",
    "  \"type\": \"object\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"restaurant_orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = build_table_schema(df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'Order Number'},\n",
       " {'type': 'string', 'description': 'Order Date'},\n",
       " {'type': 'string', 'description': 'Item Name'},\n",
       " {'type': 'integer', 'description': 'Quantity'},\n",
       " {'type': 'number', 'description': 'Product Price'},\n",
       " {'type': 'integer', 'description': 'Total products'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_schema = kafka_basic_schema.copy()\n",
    "df_final_schema['properties'] = {}\n",
    "for value in df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"Order Number\": {\"type\": \"integer\"}, \"Order Date\": {\"type\": \"string\"}, \"Item Name\": {\"type\": \"string\"}, \"Quantity\": {\"type\": \"integer\"}, \"Product Price\": {\"type\": \"number\"}, \"Total products\": {\"type\": \"integer\"}}}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(df_final_schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write a kafka producer program (python or any other language) to read data records from restaurent data csv file, \n",
    "* make sure schema is not hardcoded in the producer code, read the latest version of schema and schema_str from schema registry and use it for data serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import argparse\n",
    "from uuid import uuid4\n",
    "from six.moves import input\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.json_schema import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "API_KEY = 'IAFVAAGRB4VRBCWF'\n",
    "ENDPOINT_SCHEMA_URL  = 'https://psrc-8qyy0.eastus2.azure.confluent.cloud'\n",
    "API_SECRET_KEY = 'bFbcUILs/aj4d+xvKk47EbjUbvz3bYADSt6ucLPH6X5Yf4g1Z9N4cO+AkAHeuyDs'\n",
    "BOOTSTRAP_SERVER = 'pkc-lgwgm.eastus2.azure.confluent.cloud:9092'\n",
    "SECURITY_PROTOCOL = 'SASL_SSL'\n",
    "SSL_MACHENISM = 'PLAIN'\n",
    "SCHEMA_REGISTRY_API_KEY = 'BS3PHBXGYPRUDGFP'\n",
    "SCHEMA_REGISTRY_API_SECRET = 'gMwNVdkw1Cn5pFHmtFKrZam64E5HeDEo1dJu2hxCcUSSjaIkuscBUw6ucAlmjT2l'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sasl_conf():\n",
    "    # connection of producer to kakfa confluent  \n",
    "    sasl_conf = {'sasl.mechanism': SSL_MACHENISM,\n",
    "                 # Set to SASL_SSL to enable TLS support.\n",
    "                #  'security.protocol': 'SASL_PLAINTEXT'}\n",
    "                'bootstrap.servers':BOOTSTRAP_SERVER,\n",
    "                'security.protocol': SECURITY_PROTOCOL,\n",
    "                'sasl.username': API_KEY,\n",
    "                'sasl.password': API_SECRET_KEY\n",
    "                }\n",
    "    return sasl_conf\n",
    "\n",
    "\n",
    "def schema_config():\n",
    "    # schema registry authentication\n",
    "    return {'url':ENDPOINT_SCHEMA_URL,\n",
    "    'basic.auth.user.info':f\"{SCHEMA_REGISTRY_API_KEY}:{SCHEMA_REGISTRY_API_SECRET}\"\n",
    "    }\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    \"\"\"\n",
    "    Reports the success or failure of a message delivery.\n",
    "    Args:\n",
    "        err (KafkaError): The error that occurred on None on success.\n",
    "        msg (Message): The message that was produced or failed.\n",
    "    \"\"\"\n",
    "\n",
    "    if err is not None:\n",
    "        print(\"Delivery failed for User record {}: {}\".format(msg.key(), err))\n",
    "        return\n",
    "    print('User record {} successfully produced to {} [{}] at offset {}'.format(\n",
    "        msg.key(), msg.topic(), msg.partition(), msg.offset()))\n",
    "\n",
    "class Car: \n",
    "    # constructor  \n",
    "    def __init__(self,record:dict):\n",
    "        for k,v in record.items():\n",
    "            setattr(self,k,v)\n",
    "        \n",
    "        self.record=record\n",
    "   \n",
    "    @staticmethod\n",
    "    def dict_to_car(data:dict,ctx):\n",
    "        return Car(record=data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.record}\"\n",
    "\n",
    "def car_to_dict(car:Car, ctx):\n",
    "    \"\"\"\n",
    "    Returns a dict representation of a User instance for serialization.\n",
    "    Args:\n",
    "        user (User): User instance.\n",
    "        ctx (SerializationContext): Metadata pertaining to the serialization\n",
    "            operation.\n",
    "    Returns:\n",
    "        dict: Dict populated with user attributes to be serialized.\n",
    "    \"\"\"\n",
    "\n",
    "    # User._address must not be serialized; omit from dict\n",
    "    return car.record\n",
    "\n",
    "# read the data from csv file\n",
    "def get_car_instance(file_path, columns):\n",
    "    df=pd.read_csv(file_path)\n",
    "    df=df.iloc[:,:] \n",
    "    cars:List[Car]=[]\n",
    "    #df.replace(np.nan, '', regex=True)\n",
    "    nan_values = df.isna().any()\n",
    "    for col, val in nan_values.items():\n",
    "        #print(col, val)\n",
    "        #break\n",
    "        if val == True:\n",
    "            type_ = df[col].dtype.name\n",
    "            #print(type_)\n",
    "            if type_ == 'int64' or type_ == 'int32':\n",
    "                df[col].fillna(-99999, inplace=True)\n",
    "            elif type_ == 'float64' or type_ == 'float32':\n",
    "                df[col].fillna(-99999.9, inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(\"*miss*\", inplace=True)\n",
    "    for data in df.values:\n",
    "        car=Car(dict(zip(columns,data)))\n",
    "        cars.append(car)\n",
    "        yield car\n",
    "\n",
    "def streamingToKafka(FILE_PATH, topic, schema_id, columns, processing_col_count):\n",
    "    schema_registry_conf = schema_config()\n",
    "    schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "    schema_str = schema_registry_client.get_schema(schema_id).schema_str\n",
    "\n",
    "    string_serializer = StringSerializer('utf_8')\n",
    "    json_serializer = JSONSerializer(schema_str, schema_registry_client, car_to_dict)\n",
    "\n",
    "    producer = Producer(sasl_conf())\n",
    "\n",
    "    print(\"Producing user records to topic {}. ^C to exit.\".format(topic))\n",
    "    #while True:\n",
    "        # Serve on_delivery callbacks from previous calls to produce()\n",
    "    producer.poll(0.0)\n",
    "    try:\n",
    "        count = 1\n",
    "        #for idx,car in enumerate(get_car_instance(FILE_PATH, columns)):\n",
    "        for car in get_car_instance(FILE_PATH, columns):\n",
    "            print(car)\n",
    "            producer.produce(topic=topic,\n",
    "                            key=string_serializer(str(uuid4()), car_to_dict),\n",
    "                            value=json_serializer(car, SerializationContext(topic, MessageField.VALUE)),\n",
    "                            on_delivery=delivery_report)\n",
    "            count += 1\n",
    "\n",
    "            # adding this condition as queue gets full while processing SeoulFloating file\n",
    "            # if count%20000 == 0:\n",
    "            #     time.sleep(30)\n",
    "\n",
    "            if count > processing_col_count:\n",
    "                break\n",
    "            # if idx == 2:\n",
    "            #     break\n",
    "            #break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, discarding record...\")\n",
    "        pass\n",
    "\n",
    "    print(\"\\nFlushing records...\")\n",
    "    producer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: From producer code, publish data in Kafka Topic one by one and use dynamic key while publishing the records into the Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'restaurant_orders.csv'\n",
    "topic = \"restaurent-take-away-data\"\n",
    "schema_id = 100017\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)\n",
    "processing_col_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing user records to topic restaurent-take-away-data. ^C to exit.\n",
      "{'Order Number': 16118, 'Order Date': '03/08/2019 20:25', 'Item Name': 'Plain Papadum', 'Quantity': 2, 'Product Price': 0.8, 'Total products': 6}\n",
      "{'Order Number': 16118, 'Order Date': '03/08/2019 20:25', 'Item Name': 'King Prawn Balti', 'Quantity': 1, 'Product Price': 12.95, 'Total products': 6}\n",
      "\n",
      "Flushing records...\n",
      "User record b'a29102e6-8e37-4dd4-a39d-58c0999fd237' successfully produced to restaurent-take-away-data [2] at offset 0\n",
      "User record b'cb9fb424-96e4-48c5-9143-1d69211138cf' successfully produced to restaurent-take-away-data [1] at offset 0\n"
     ]
    }
   ],
   "source": [
    "streamingToKafka(FILE_PATH, topic, schema_id, cols, processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Write kafka consumer code and create two copies of same consumer code and save it with different names (kafka_consumer_1.py & kafka_consumer_2.py), \n",
    "* again make sure lates schema version and schema_str is not hardcoded in the consumer code, read it automatically from the schema registry to desrialize the data. <br>\n",
    "* Now test two scenarios with your consumer code:\n",
    "    1. Use \"group.id\" property in consumer config for both consumers and mention different group_ids in kafka_consumer_1.py & kafka_consumer_2.py,<br>\n",
    "        apply \"earliest\" offset property in both consumers and run these two consumers from two different terminals. Calculate how many records each consumer\n",
    "        consumed and printed on the terminal\n",
    "    2. Use \"group.id\" property in consumer config for both consumers and mention same group_ids in kafka_consumer_1.py & kafka_consumer_2.py,<br>\n",
    "        apply \"earliest\" offset property in both consumers and run these two consumers from two different terminals. Calculate how many records each consumer\n",
    "        consumed and printed on the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from confluent_kafka import Consumer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry.json_schema import JSONDeserializer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka_consumer_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sasl_conf():\n",
    "\n",
    "    sasl_conf = {'sasl.mechanism': SSL_MACHENISM,\n",
    "                 # Set to SASL_SSL to enable TLS support.\n",
    "                #  'security.protocol': 'SASL_PLAINTEXT'}\n",
    "                'bootstrap.servers':BOOTSTRAP_SERVER,\n",
    "                'security.protocol': SECURITY_PROTOCOL,\n",
    "                'sasl.username': API_KEY,\n",
    "                'sasl.password': API_SECRET_KEY\n",
    "                }\n",
    "    return sasl_conf\n",
    "\n",
    "def schema_config():\n",
    "    return {'url':ENDPOINT_SCHEMA_URL,\n",
    "    'basic.auth.user.info':f\"{SCHEMA_REGISTRY_API_KEY}:{SCHEMA_REGISTRY_API_SECRET}\"\n",
    "    }\n",
    "\n",
    "class Car:   \n",
    "    def __init__(self,record:dict):\n",
    "        for k,v in record.items():\n",
    "            setattr(self,k,v)\n",
    "        \n",
    "        self.record=record\n",
    "   \n",
    "    @staticmethod\n",
    "    def dict_to_car(data:dict,ctx):\n",
    "        return Car(record=data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.record}\"\n",
    "\n",
    "\n",
    "def kafka_consumer(topic, schema_id):\n",
    "    schema_registry_conf = schema_config()\n",
    "    schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "    print('reading schema')\n",
    "    schema_str = schema_registry_client.get_schema(schema_id).schema_str\n",
    "    print(schema_str)\n",
    "    json_deserializer = JSONDeserializer(schema_str,\n",
    "                                         from_dict=Car.dict_to_car)\n",
    "\n",
    "    print(\"deserializer start\")\n",
    "    consumer_conf = sasl_conf()\n",
    "    consumer_conf.update({\n",
    "                     'group.id': 'group1',\n",
    "                     'auto.offset.reset': \"latest\"})\n",
    "\n",
    "    print('consumer')\n",
    "    consumer = Consumer(consumer_conf)\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
    "            msg = consumer.poll(1.0)\n",
    "            if msg is None:\n",
    "                continue\n",
    "\n",
    "            car = json_deserializer(msg.value(), SerializationContext(msg.topic(), MessageField.VALUE))\n",
    "\n",
    "            if car is not None:\n",
    "                print(\"User record {}: car: {}\\n\"\n",
    "                      .format(msg.key(), car))\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'topic_case'\n",
    "schema_id = 100005\n",
    "\n",
    "kafka_consumer(topic, schema_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka_consumer_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Once above questions are done, write another kafka consumer to read data from kafka topic and from the consumer code create one csv file \"output.csv\" and append consumed records output.csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
