{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Project\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset** - https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset\n",
    "\n",
    "Step 1 - Create a producer with a python connector in confluent kafka and\n",
    "stream your data.\n",
    "\n",
    "Step 2 - Consume your data through the python connector and dump it in\n",
    "mongodb atlas.\n",
    "Note: Here in the dataset you will be finding a multiple files you\n",
    "need to use all file for the kafka and mongodb\n",
    "\n",
    "Step 3 - Collect your data as a pyspark dataframe and perform different\n",
    "operations.<br>\n",
    "Note: Consider only three files for creating a dataframe among all\n",
    "case, region and TimeProvince<br>\n",
    "1. Read the data, show it and Count the number of records.\n",
    "2. Describe the data with a describe function.\n",
    "3. If there is any duplicate value drop it.\n",
    "4. Use limit function for showcasing a limited number of\n",
    "records.\n",
    "5. If you find the column name is not suitable, change the\n",
    "column name.[optional]\n",
    "6. Select the subset of the columns.\n",
    "7. If there is any null value, fill it with any random value or drop\n",
    "it.\n",
    "8. Filter the data based on different columns or variables and\n",
    "do the best analysis.\n",
    "<br>For example: We can filter a data frame using multiple\n",
    "conditions using AND(&), OR(|) and NOT(~) conditions. For\n",
    "example, we may want to find out all the dif erent\n",
    "infection_case in Daegu Province with more than 10\n",
    "confirmed cases.</br>\n",
    "9. Sort the number of confirmed cases. Confirmed column is\n",
    "there in the dataset. Check with descending sort also.\n",
    "10. In case of any wrong data type, cast that data type from\n",
    "integer to string or string to integer.\n",
    "Use group by on top of province and city column and agg it\n",
    "with sum of confirmed cases. For example\n",
    "df.groupBy([\"province\",\"city\"]).agg(function.sum(\"co\n",
    "nfirmed\")\n",
    "11. For joins we will need one more file.you can use region file.\n",
    "User different different join methods.for example\n",
    "cases.join(regions, ['province','city'],how='left')\n",
    "You can do your best analysis.\n",
    "\n",
    "Step 4 - If you want, you can also use SQL with data frames. Let us try to\n",
    "run some SQL on the cases table.<br>\n",
    "For example:<br>\n",
    "cases.registerTempTable('cases_table')<br>\n",
    "newDF = sqlContext.sql('select * from cases_table where\n",
    "confirmed>100')<br>\n",
    "newDF.show()\n",
    "<br>\n",
    "<t>\n",
    "<br>\n",
    "Here is a example how you can use df for sql now you can perform\n",
    "various operations with GROUP BY, HAVING, AND ORDER BY\n",
    "\n",
    "Step 5 - Create Spark UDFs\n",
    "Create function casehighlow()<br>\n",
    "If case is less than 50 return low else return high<br>\n",
    "convert into a UDF Function and mention the return type of\n",
    "function.<br>\n",
    "Note: You can create as many as udf based on analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create a producer with a python connector in confluent kafka and stream your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all needed packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import argparse\n",
    "from uuid import uuid4\n",
    "from six.moves import input\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.json_schema import JSONSerializer\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import build_table_schema\n",
    "import json\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka essentials details\n",
    "\n",
    "# Variables\n",
    "API_KEY = 'AMTQUJ4OYGGMNYOJ'\n",
    "ENDPOINT_SCHEMA_URL  = 'https://psrc-8qyy0.eastus2.azure.confluent.cloud'\n",
    "API_SECRET_KEY = '4HTAJgLsRtyeQjCpKZItrYiCs7eduapoIzAkFApIZer4nzPQ2i53tPWe58TGuPY/'\n",
    "BOOTSTRAP_SERVER = 'pkc-n00kk.us-east-1.aws.confluent.cloud:9092'\n",
    "SECURITY_PROTOCOL = 'SASL_SSL'\n",
    "SSL_MACHENISM = 'PLAIN'\n",
    "SCHEMA_REGISTRY_API_KEY = 'BS3PHBXGYPRUDGFP'\n",
    "SCHEMA_REGISTRY_API_SECRET = 'gMwNVdkw1Cn5pFHmtFKrZam64E5HeDEo1dJu2hxCcUSSjaIkuscBUw6ucAlmjT2l'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Producer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sasl_conf():\n",
    "    # connection of producer to kakfa confluent  \n",
    "    sasl_conf = {'sasl.mechanism': SSL_MACHENISM,\n",
    "                 # Set to SASL_SSL to enable TLS support.\n",
    "                #  'security.protocol': 'SASL_PLAINTEXT'}\n",
    "                'bootstrap.servers':BOOTSTRAP_SERVER,\n",
    "                'security.protocol': SECURITY_PROTOCOL,\n",
    "                'sasl.username': API_KEY,\n",
    "                'sasl.password': API_SECRET_KEY\n",
    "                }\n",
    "    return sasl_conf\n",
    "\n",
    "\n",
    "def schema_config():\n",
    "    # schema registry authentication\n",
    "    return {'url':ENDPOINT_SCHEMA_URL,\n",
    "    'basic.auth.user.info':f\"{SCHEMA_REGISTRY_API_KEY}:{SCHEMA_REGISTRY_API_SECRET}\"\n",
    "    }\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    \"\"\"\n",
    "    Reports the success or failure of a message delivery.\n",
    "    Args:\n",
    "        err (KafkaError): The error that occurred on None on success.\n",
    "        msg (Message): The message that was produced or failed.\n",
    "    \"\"\"\n",
    "\n",
    "    if err is not None:\n",
    "        print(\"Delivery failed for User record {}: {}\".format(msg.key(), err))\n",
    "        return\n",
    "    print('User record {} successfully produced to {} [{}] at offset {}'.format(\n",
    "        msg.key(), msg.topic(), msg.partition(), msg.offset()))\n",
    "\n",
    "class Car: \n",
    "    # constructor  \n",
    "    def __init__(self,record:dict):\n",
    "        for k,v in record.items():\n",
    "            setattr(self,k,v)\n",
    "        \n",
    "        self.record=record\n",
    "   \n",
    "    @staticmethod\n",
    "    def dict_to_car(data:dict,ctx):\n",
    "        return Car(record=data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.record}\"\n",
    "\n",
    "def car_to_dict(car:Car, ctx):\n",
    "    \"\"\"\n",
    "    Returns a dict representation of a User instance for serialization.\n",
    "    Args:\n",
    "        user (User): User instance.\n",
    "        ctx (SerializationContext): Metadata pertaining to the serialization\n",
    "            operation.\n",
    "    Returns:\n",
    "        dict: Dict populated with user attributes to be serialized.\n",
    "    \"\"\"\n",
    "\n",
    "    # User._address must not be serialized; omit from dict\n",
    "    return car.record\n",
    "\n",
    "# read the data from csv file\n",
    "def get_car_instance(file_path, columns):\n",
    "    df=pd.read_csv(file_path)\n",
    "    df=df.iloc[:,:] \n",
    "    cars:List[Car]=[]\n",
    "    #df.replace(np.nan, '', regex=True)\n",
    "    nan_values = df.isna().any()\n",
    "    for col, val in nan_values.items():\n",
    "        #print(col, val)\n",
    "        #break\n",
    "        if val == True:\n",
    "            type_ = df[col].dtype.name\n",
    "            #print(type_)\n",
    "            if type_ == 'int64' or type_ == 'int32':\n",
    "                df[col].fillna(-99999, inplace=True)\n",
    "            elif type_ == 'float64' or type_ == 'float32':\n",
    "                df[col].fillna(-99999.9, inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(\"*miss*\", inplace=True)\n",
    "    for data in df.values:\n",
    "        car=Car(dict(zip(columns,data)))\n",
    "        cars.append(car)\n",
    "        yield car\n",
    "\n",
    "def streamingToKafka(FILE_PATH, topic, schema_id, columns, processing_col_count):\n",
    "    schema_registry_conf = schema_config()\n",
    "    schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "    schema_str = schema_registry_client.get_schema(schema_id).schema_str\n",
    "\n",
    "    string_serializer = StringSerializer('utf_8')\n",
    "    json_serializer = JSONSerializer(schema_str, schema_registry_client, car_to_dict)\n",
    "\n",
    "    producer = Producer(sasl_conf())\n",
    "\n",
    "    print(\"Producing user records to topic {}. ^C to exit.\".format(topic))\n",
    "    #while True:\n",
    "        # Serve on_delivery callbacks from previous calls to produce()\n",
    "    producer.poll(0.0)\n",
    "    try:\n",
    "        count = 1\n",
    "        #for idx,car in enumerate(get_car_instance(FILE_PATH, columns)):\n",
    "        for car in get_car_instance(FILE_PATH, columns):\n",
    "            print(car)\n",
    "            producer.produce(topic=topic,\n",
    "                            key=string_serializer(str(uuid4()), car_to_dict),\n",
    "                            value=json_serializer(car, SerializationContext(topic, MessageField.VALUE)),\n",
    "                            on_delivery=delivery_report)\n",
    "            count += 1\n",
    "\n",
    "            # adding this condition as queue gets full while processing SeoulFloating file\n",
    "            # if count%20000 == 0:\n",
    "            #     time.sleep(30)\n",
    "\n",
    "            if count > processing_col_count:\n",
    "                break\n",
    "            # if idx == 2:\n",
    "            #     break\n",
    "            #break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, discarding record...\")\n",
    "        pass\n",
    "\n",
    "    print(\"\\nFlushing records...\")\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added:  Case.csv\n",
      "Added:  PatientInfo.csv\n",
      "Added:  Policy.csv\n",
      "Added:  Region.csv\n",
      "Added:  SearchTrend.csv\n",
      "Added:  SeoulFloating.csv\n",
      "Added:  Time.csv\n",
      "Added:  TimeAge.csv\n",
      "Added:  TimeGender.csv\n",
      "Added:  TimeProvince.csv\n",
      "Added:  Weather.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name_list = []\n",
    "file_name = \"dataset.zip\"\n",
    "with ZipFile(file_name, 'r') as zipfile:\n",
    "    for zipInfo in zip.filelist:\n",
    "        print(\"Added: \", zipInfo.filename)\n",
    "        file_name_list.append(zipInfo.filename)\n",
    "    #zip.extractall()\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_basic_schema = {\n",
    "  \"$id\": \"http://example.com/myURI.schema.json\",\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"additionalProperties\": False,\n",
    "  \"description\": \"Sample schema to help you get started.\",\n",
    "  \"title\": \"SampleRecord\",\n",
    "  \"type\": \"object\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = pd.read_csv('Case.csv')\n",
    "case_df_schema = build_table_schema(case_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'case_id'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'string', 'description': 'city'},\n",
       " {'type': 'boolean', 'description': 'group'},\n",
       " {'type': 'string', 'description': 'infection_case'},\n",
       " {'type': 'integer', 'description': 'confirmed'},\n",
       " {'type': 'string', 'description': 'latitude'},\n",
       " {'type': 'string', 'description': 'longitude'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in case_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "case_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df_final_schema = kafka_basic_schema.copy()\n",
    "case_df_final_schema['properties'] = {}\n",
    "for value in case_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    case_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$id': 'http://example.com/myURI.schema.json',\n",
       " '$schema': 'http://json-schema.org/draft-07/schema#',\n",
       " 'additionalProperties': False,\n",
       " 'description': 'Sample schema to help you get started.',\n",
       " 'title': 'SampleRecord',\n",
       " 'type': 'object',\n",
       " 'properties': {'case_id': {'type': 'integer'},\n",
       "  'province': {'type': 'string'},\n",
       "  'city': {'type': 'string'},\n",
       "  'group': {'type': 'boolean'},\n",
       "  'infection_case': {'type': 'string'},\n",
       "  'confirmed': {'type': 'integer'},\n",
       "  'latitude': {'type': 'string'},\n",
       "  'longitude': {'type': 'string'}}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_df_final_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_registry_conf = schema_config()\n",
    "# schema_registry_client = SchemaRegistryClient(schema_registry_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema.schema_str = json.dumps(case_df_final_schema)\n",
    "# Schema.schema_type = 'JSON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = schema_registry_client.register_schema(\"case_schema\", Schema)\n",
    "# id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'Case.csv'\n",
    "topic = \"topic_case\"\n",
    "schema_id = 100005\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingToKafka(FILE_PATH, topic, schema_id, cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatientInfo CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info_df = pd.read_csv('PatientInfo.csv')\n",
    "patient_info_df_schema = build_table_schema(patient_info_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'patient_id'},\n",
       " {'type': 'string', 'description': 'sex'},\n",
       " {'type': 'string', 'description': 'age'},\n",
       " {'type': 'string', 'description': 'country'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'string', 'description': 'city'},\n",
       " {'type': 'string', 'description': 'infection_case'},\n",
       " {'type': 'string', 'description': 'infected_by'},\n",
       " {'type': 'string', 'description': 'contact_number'},\n",
       " {'type': 'string', 'description': 'symptom_onset_date'},\n",
       " {'type': 'string', 'description': 'confirmed_date'},\n",
       " {'type': 'string', 'description': 'released_date'},\n",
       " {'type': 'string', 'description': 'deceased_date'},\n",
       " {'type': 'string', 'description': 'state'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in patient_info_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "patient_info_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info_df_final_schema = kafka_basic_schema.copy()\n",
    "patient_info_df_final_schema['properties'] = {}\n",
    "for value in patient_info_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    patient_info_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"patient_id\": {\"type\": \"integer\"}, \"sex\": {\"type\": \"string\"}, \"age\": {\"type\": \"string\"}, \"country\": {\"type\": \"string\"}, \"province\": {\"type\": \"string\"}, \"city\": {\"type\": \"string\"}, \"infection_case\": {\"type\": \"string\"}, \"infected_by\": {\"type\": \"string\"}, \"contact_number\": {\"type\": \"string\"}, \"symptom_onset_date\": {\"type\": \"string\"}, \"confirmed_date\": {\"type\": \"string\"}, \"released_date\": {\"type\": \"string\"}, \"deceased_date\": {\"type\": \"string\"}, \"state\": {\"type\": \"string\"}}}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(patient_info_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id            False\n",
       "sex                   False\n",
       "age                   False\n",
       "country               False\n",
       "province              False\n",
       "city                  False\n",
       "infection_case        False\n",
       "infected_by           False\n",
       "contact_number        False\n",
       "symptom_onset_date    False\n",
       "confirmed_date        False\n",
       "released_date         False\n",
       "deceased_date         False\n",
       "state                 False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_info_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'PatientInfo.csv'\n",
    "topic = \"topic_patientinfo\"\n",
    "schema_id = 100006\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_df = pd.read_csv('Policy.csv')\n",
    "policy_df_schema = build_table_schema(policy_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'policy_id'},\n",
       " {'type': 'string', 'description': 'country'},\n",
       " {'type': 'string', 'description': 'type'},\n",
       " {'type': 'string', 'description': 'gov_policy'},\n",
       " {'type': 'string', 'description': 'detail'},\n",
       " {'type': 'string', 'description': 'start_date'},\n",
       " {'type': 'string', 'description': 'end_date'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in policy_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "policy_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_df_final_schema = kafka_basic_schema.copy()\n",
    "policy_df_final_schema['properties'] = {}\n",
    "for value in policy_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    policy_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"policy_id\": {\"type\": \"integer\"}, \"country\": {\"type\": \"string\"}, \"type\": {\"type\": \"string\"}, \"gov_policy\": {\"type\": \"string\"}, \"detail\": {\"type\": \"string\"}, \"start_date\": {\"type\": \"string\"}, \"end_date\": {\"type\": \"string\"}}}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(policy_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_id     False\n",
       "country       False\n",
       "type          False\n",
       "gov_policy    False\n",
       "detail         True\n",
       "start_date    False\n",
       "end_date       True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'Policy.csv'\n",
    "topic = \"topic_policy\"\n",
    "schema_id = 100008\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "processing_col_count = 1000000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = pd.read_csv('Region.csv')\n",
    "region_df_schema = build_table_schema(region_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'code'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'string', 'description': 'city'},\n",
       " {'type': 'number', 'description': 'latitude'},\n",
       " {'type': 'number', 'description': 'longitude'},\n",
       " {'type': 'integer', 'description': 'elementary_school_count'},\n",
       " {'type': 'integer', 'description': 'kindergarten_count'},\n",
       " {'type': 'integer', 'description': 'university_count'},\n",
       " {'type': 'number', 'description': 'academy_ratio'},\n",
       " {'type': 'number', 'description': 'elderly_population_ratio'},\n",
       " {'type': 'number', 'description': 'elderly_alone_ratio'},\n",
       " {'type': 'integer', 'description': 'nursing_home_count'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in region_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "region_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating schema for kafka topic\n",
    "region_df_final_schema = kafka_basic_schema.copy()\n",
    "region_df_final_schema['properties'] = {}\n",
    "for value in region_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    region_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"code\": {\"type\": \"integer\"}, \"province\": {\"type\": \"string\"}, \"city\": {\"type\": \"string\"}, \"latitude\": {\"type\": \"number\"}, \"longitude\": {\"type\": \"number\"}, \"elementary_school_count\": {\"type\": \"integer\"}, \"kindergarten_count\": {\"type\": \"integer\"}, \"university_count\": {\"type\": \"integer\"}, \"academy_ratio\": {\"type\": \"number\"}, \"elderly_population_ratio\": {\"type\": \"number\"}, \"elderly_alone_ratio\": {\"type\": \"number\"}, \"nursing_home_count\": {\"type\": \"integer\"}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(region_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                        False\n",
       "province                    False\n",
       "city                        False\n",
       "latitude                    False\n",
       "longitude                   False\n",
       "elementary_school_count     False\n",
       "kindergarten_count          False\n",
       "university_count            False\n",
       "academy_ratio               False\n",
       "elderly_population_ratio    False\n",
       "elderly_alone_ratio         False\n",
       "nursing_home_count          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking columns having null values\n",
    "region_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'Region.csv'\n",
    "topic = \"topic_region\"\n",
    "schema_id = 100009\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "processing_col_count = 10000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SearchTrend CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchtrend_df = pd.read_csv('SearchTrend.csv')\n",
    "searchtrend_df_schema = build_table_schema(searchtrend_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'string', 'description': 'date'},\n",
       " {'type': 'number', 'description': 'cold'},\n",
       " {'type': 'number', 'description': 'flu'},\n",
       " {'type': 'number', 'description': 'pneumonia'},\n",
       " {'type': 'number', 'description': 'coronavirus'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in searchtrend_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "searchtrend_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating schema for kafka topic\n",
    "searchtrend_df_final_schema = kafka_basic_schema.copy()\n",
    "searchtrend_df_final_schema['properties'] = {}\n",
    "for value in searchtrend_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    searchtrend_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"date\": {\"type\": \"string\"}, \"cold\": {\"type\": \"number\"}, \"flu\": {\"type\": \"number\"}, \"pneumonia\": {\"type\": \"number\"}, \"coronavirus\": {\"type\": \"number\"}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(searchtrend_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date           False\n",
       "cold           False\n",
       "flu            False\n",
       "pneumonia      False\n",
       "coronavirus    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking columns having null values\n",
    "searchtrend_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'SearchTrend.csv'\n",
    "topic = \"topic_searchtrend\"\n",
    "schema_id = 100010\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "processing_col_count = 1000000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SeoulFloating CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seoulfloating_df = pd.read_csv('SeoulFloating.csv')\n",
    "seoulfloating_df_schema = build_table_schema(seoulfloating_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'string', 'description': 'date'},\n",
       " {'type': 'integer', 'description': 'hour'},\n",
       " {'type': 'integer', 'description': 'birth_year'},\n",
       " {'type': 'string', 'description': 'sex'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'string', 'description': 'city'},\n",
       " {'type': 'integer', 'description': 'fp_num'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in seoulfloating_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "seoulfloating_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating schema for kafka topic\n",
    "seoulfloating_df_final_schema = kafka_basic_schema.copy()\n",
    "seoulfloating_df_final_schema['properties'] = {}\n",
    "for value in seoulfloating_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    seoulfloating_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"date\": {\"type\": \"string\"}, \"hour\": {\"type\": \"integer\"}, \"birth_year\": {\"type\": \"integer\"}, \"sex\": {\"type\": \"string\"}, \"province\": {\"type\": \"string\"}, \"city\": {\"type\": \"string\"}, \"fp_num\": {\"type\": \"integer\"}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(seoulfloating_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date          False\n",
       "hour          False\n",
       "birth_year    False\n",
       "sex           False\n",
       "province      False\n",
       "city          False\n",
       "fp_num        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking columns having null values\n",
    "seoulfloating_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'SeoulFloating.csv'\n",
    "topic = \"topic_seoulfloating\"\n",
    "schema_id = 100011\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date          1084800\n",
       "hour          1084800\n",
       "birth_year    1084800\n",
       "sex           1084800\n",
       "province      1084800\n",
       "city          1084800\n",
       "fp_num        1084800\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seoulfloating_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "processing_col_count = 1000000000000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.read_csv('Time.csv')\n",
    "time_df_schema = build_table_schema(time_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'string', 'description': 'date'},\n",
       " {'type': 'integer', 'description': 'time'},\n",
       " {'type': 'integer', 'description': 'test'},\n",
       " {'type': 'integer', 'description': 'negative'},\n",
       " {'type': 'integer', 'description': 'confirmed'},\n",
       " {'type': 'integer', 'description': 'released'},\n",
       " {'type': 'integer', 'description': 'deceased'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in time_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "time_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating schema for kafka topic\n",
    "time_df_final_schema = kafka_basic_schema.copy()\n",
    "time_df_final_schema['properties'] = {}\n",
    "for value in time_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    time_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"date\": {\"type\": \"string\"}, \"time\": {\"type\": \"integer\"}, \"test\": {\"type\": \"integer\"}, \"negative\": {\"type\": \"integer\"}, \"confirmed\": {\"type\": \"integer\"}, \"released\": {\"type\": \"integer\"}, \"deceased\": {\"type\": \"integer\"}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(time_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         False\n",
       "time         False\n",
       "test         False\n",
       "negative     False\n",
       "confirmed    False\n",
       "released     False\n",
       "deceased     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking columns having null values\n",
    "time_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'Time.csv'\n",
    "topic = \"topic_time\"\n",
    "schema_id = 100012\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         163\n",
       "time         163\n",
       "test         163\n",
       "negative     163\n",
       "confirmed    163\n",
       "released     163\n",
       "deceased     163\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "processing_col_count = 10000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeAge CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeage_df = pd.read_csv('TimeAge.csv')\n",
    "timeage_df_schema = build_table_schema(timeage_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'string', 'description': 'date'},\n",
       " {'type': 'integer', 'description': 'time'},\n",
       " {'type': 'string', 'description': 'age'},\n",
       " {'type': 'integer', 'description': 'confirmed'},\n",
       " {'type': 'integer', 'description': 'deceased'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in timeage_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "timeage_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating schema for kafka topic\n",
    "timeage_df_final_schema = kafka_basic_schema.copy()\n",
    "timeage_df_final_schema['properties'] = {}\n",
    "for value in timeage_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    timeage_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"date\": {\"type\": \"string\"}, \"time\": {\"type\": \"integer\"}, \"age\": {\"type\": \"string\"}, \"confirmed\": {\"type\": \"integer\"}, \"deceased\": {\"type\": \"integer\"}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(timeage_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         False\n",
       "time         False\n",
       "age          False\n",
       "confirmed    False\n",
       "deceased     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking columns having null values\n",
    "timeage_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'TimeAge.csv'\n",
    "topic = \"topic_timeage\"\n",
    "schema_id = 100013\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         1089\n",
       "time         1089\n",
       "age          1089\n",
       "confirmed    1089\n",
       "deceased     1089\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeage_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "processing_col_count = 10000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeGender CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegender_df = pd.read_csv('TimeGender.csv')\n",
    "timegender_df_schema = build_table_schema(timegender_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'string', 'description': 'date'},\n",
       " {'type': 'integer', 'description': 'time'},\n",
       " {'type': 'string', 'description': 'sex'},\n",
       " {'type': 'integer', 'description': 'confirmed'},\n",
       " {'type': 'integer', 'description': 'deceased'}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in timegender_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "timegender_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating schema for kafka topic\n",
    "timegender_df_final_schema = kafka_basic_schema.copy()\n",
    "timegender_df_final_schema['properties'] = {}\n",
    "for value in timegender_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    timegender_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"date\": {\"type\": \"string\"}, \"time\": {\"type\": \"integer\"}, \"sex\": {\"type\": \"string\"}, \"confirmed\": {\"type\": \"integer\"}, \"deceased\": {\"type\": \"integer\"}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(timegender_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         False\n",
       "time         False\n",
       "sex          False\n",
       "confirmed    False\n",
       "deceased     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking columns having null values\n",
    "timegender_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'TimeGender.csv'\n",
    "topic = \"topic_timegender\"\n",
    "schema_id = 100014\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         242\n",
       "time         242\n",
       "sex          242\n",
       "confirmed    242\n",
       "deceased     242\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timegender_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "processing_col_count = 10000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeProvince CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeprovince_df = pd.read_csv('TimeProvince.csv')\n",
    "timeprovince_df_schema = build_table_schema(timeprovince_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'string', 'description': 'date'},\n",
       " {'type': 'integer', 'description': 'time'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'integer', 'description': 'confirmed'},\n",
       " {'type': 'integer', 'description': 'released'},\n",
       " {'type': 'integer', 'description': 'deceased'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in timeprovince_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "timeprovince_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating schema for kafka topic\n",
    "timeprovince_df_final_schema = kafka_basic_schema.copy()\n",
    "timeprovince_df_final_schema['properties'] = {}\n",
    "for value in timeprovince_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    timeprovince_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"date\": {\"type\": \"string\"}, \"time\": {\"type\": \"integer\"}, \"province\": {\"type\": \"string\"}, \"confirmed\": {\"type\": \"integer\"}, \"released\": {\"type\": \"integer\"}, \"deceased\": {\"type\": \"integer\"}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(timeprovince_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         False\n",
       "time         False\n",
       "province     False\n",
       "confirmed    False\n",
       "released     False\n",
       "deceased     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking columns having null values\n",
    "timeprovince_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'TimeProvince.csv'\n",
    "topic = \"topic_timeprovince\"\n",
    "schema_id = 100015\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date         2771\n",
       "time         2771\n",
       "province     2771\n",
       "confirmed    2771\n",
       "released     2771\n",
       "deceased     2771\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeprovince_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "processing_col_count = 10000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('Weather.csv')\n",
    "weather_df_schema = build_table_schema(weather_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'code'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'string', 'description': 'date'},\n",
       " {'type': 'number', 'description': 'avg_temp'},\n",
       " {'type': 'number', 'description': 'min_temp'},\n",
       " {'type': 'number', 'description': 'max_temp'},\n",
       " {'type': 'number', 'description': 'precipitation'},\n",
       " {'type': 'number', 'description': 'max_wind_speed'},\n",
       " {'type': 'number', 'description': 'most_wind_direction'},\n",
       " {'type': 'number', 'description': 'avg_relative_humidity'}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in weather_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "weather_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating schema for kafka topic\n",
    "weather_df_final_schema = kafka_basic_schema.copy()\n",
    "weather_df_final_schema['properties'] = {}\n",
    "for value in weather_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    weather_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"code\": {\"type\": \"integer\"}, \"province\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"}, \"avg_temp\": {\"type\": \"number\"}, \"min_temp\": {\"type\": \"number\"}, \"max_temp\": {\"type\": \"number\"}, \"precipitation\": {\"type\": \"number\"}, \"max_wind_speed\": {\"type\": \"number\"}, \"most_wind_direction\": {\"type\": \"number\"}, \"avg_relative_humidity\": {\"type\": \"number\"}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.dumps(weather_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                     False\n",
       "province                 False\n",
       "date                     False\n",
       "avg_temp                  True\n",
       "min_temp                  True\n",
       "max_temp                  True\n",
       "precipitation            False\n",
       "max_wind_speed            True\n",
       "most_wind_direction       True\n",
       "avg_relative_humidity     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking columns having null values\n",
    "weather_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'Weather.csv'\n",
    "topic = \"topic_weather\"\n",
    "schema_id = 100016\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code                     26271\n",
       "province                 26271\n",
       "date                     26271\n",
       "avg_temp                 26256\n",
       "min_temp                 26266\n",
       "max_temp                 26268\n",
       "precipitation            26271\n",
       "max_wind_speed           26262\n",
       "most_wind_direction      26242\n",
       "avg_relative_humidity    26251\n",
       "dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1674547788.716|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 3645106ms in state UP)\n",
      "%6|1674548849.684|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1058012ms in state UP)\n",
      "%6|1674548850.013|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1059490ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674548850.320|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1059604ms in state UP)\n",
      "%4|1674548850.608|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1059646ms in state UP)\n",
      "%6|1674549908.576|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1056200ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674549908.878|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1057261ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674549909.116|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1056535ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674549909.354|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1056314ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674549909.653|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1057340ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674549909.946|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1057929ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674550919.732|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1009351ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674550919.993|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1009207ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674550920.218|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1009404ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674550920.480|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1009051ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674550920.765|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1010056ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674550921.037|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1009229ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674550921.342|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1009385ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674551939.516|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1017411ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674551939.829|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1017023ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674551940.120|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1016877ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674551940.367|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1019030ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674551940.678|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1017725ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674551940.982|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1018542ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674551941.297|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1019539ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674552933.428|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 991337ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674552933.651|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 990859ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674552933.878|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 992851ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674552934.092|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 992185ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674552934.386|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 991392ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674552934.657|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 992125ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674552934.929|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 993245ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674553876.505|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 940365ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674553876.801|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 941396ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674553877.045|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 941388ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674553877.447|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 940585ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674553877.746|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 940784ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674553878.061|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 942518ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674553878.367|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 941926ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674553878.673|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 943023ms in state UP, 1 identical error(s) suppressed)\n",
      "%5|1674554955.442|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Timed out MetadataRequest in flight (after 1074528ms, timeout #0)\n",
      "%4|1674554955.442|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Timed out 1 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n",
      "%3|1674554955.443|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: 1 request(s) timed out: disconnect (after 1074529ms in state UP)\n",
      "%4|1674554955.530|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1076525ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674554955.828|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1076288ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674554956.128|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 1076461ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674554956.438|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1077974ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674554956.728|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1076467ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674554956.971|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1077303ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674554957.241|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1078429ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674556005.561|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1048434ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674556005.872|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1048034ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674556006.152|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1047328ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674556006.451|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 1048348ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674556006.790|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1049243ms in state UP)\n",
      "%6|1674556007.080|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1048052ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674556007.294|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1049121ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674557071.394|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 2113135ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674557071.650|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1064384ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674557071.892|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1063531ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674557072.173|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 1063732ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674557072.401|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1063695ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674557072.755|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1064618ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674557073.037|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1063919ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674557073.331|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1065216ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674558155.465|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1080198ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674558155.764|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1080709ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674558156.062|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1082368ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674558156.311|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1082156ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674558156.537|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1083052ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674558156.796|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 1082785ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674558157.133|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1082209ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674558157.396|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 1082305ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674558157.696|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1084194ms in state UP, 1 identical error(s) suppressed)\n",
      "%5|1674559196.839|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Timed out MetadataRequest in flight (after 1037474ms, timeout #0)\n",
      "%4|1674559196.839|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Timed out 1 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n",
      "%3|1674559196.840|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: 1 request(s) timed out: disconnect (after 1037475ms in state UP)\n",
      "%4|1674559197.423|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1038290ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674559197.689|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1038061ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674559197.965|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1040366ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674559198.261|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1040768ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674559198.537|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 1039873ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674559198.781|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1041099ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674559199.104|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1040439ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674560179.354|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 978844ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674560179.651|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 978873ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674560179.960|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 980914ms in state UP)\n",
      "%4|1674560180.218|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 979436ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674560180.518|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 981165ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674560180.825|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 980860ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674560181.124|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 981434ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674561248.195|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1065561ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674561248.495|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1066040ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674561248.712|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1067634ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674561248.974|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 1067869ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674561249.249|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 1067434ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674561249.540|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1067410ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674561249.769|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 3091873ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674561250.046|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 2049950ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674561250.338|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1067319ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674562216.435|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 966378ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674562216.663|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 965843ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674562216.937|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 964805ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674562217.181|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 966952ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674562217.466|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 966898ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674562217.781|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 965877ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674562218.025|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 966284ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674562218.286|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 967000ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674563275.188|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 1055519ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674563275.427|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1057381ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674563275.785|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1056940ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674563276.006|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 2024763ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674563276.263|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1057188ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674563276.545|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1056524ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674563276.848|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1057844ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674563277.124|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1057291ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674564181.180|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 902379ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674564181.425|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 902802ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674564181.681|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 904704ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674564181.953|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 1963471ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674564182.176|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 904414ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674564182.405|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 905362ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674564182.713|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 904569ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674564183.007|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 905276ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674565135.197|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 951385ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674565135.461|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 952610ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674565135.701|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 952411ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674565135.988|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 951096ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674565136.263|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 952089ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674565136.550|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 953329ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674565136.774|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1858720ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674565137.043|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 953320ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674565137.340|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 952361ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674566054.401|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 916875ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674566054.620|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 917118ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674566054.913|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 915652ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674566055.178|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 916378ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674566055.469|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 917312ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674566055.721|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 917566ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674566055.976|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 917521ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674566056.243|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 919135ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674566056.539|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 919399ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674567039.315|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 982905ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674567039.589|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 982569ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674567039.815|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 981748ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674567040.116|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 983215ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674567040.441|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 983790ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674567040.738|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 983352ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674567041.016|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 982922ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674567041.271|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 983817ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674567041.594|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 983497ms in state UP, 1 identical error(s) suppressed)\n",
      "%5|1674568025.566|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Timed out MetadataRequest in flight (after 982253ms, timeout #0)\n",
      "%5|1674568025.566|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Timed out MetadataRequest in flight (after 982253ms, timeout #1)\n",
      "%4|1674568025.566|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Timed out 2 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n",
      "%3|1674568025.566|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: 2 request(s) timed out: disconnect (after 982253ms in state UP)\n",
      "%5|1674568025.985|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Timed out MetadataRequest in flight (after 982484ms, timeout #0)\n",
      "%4|1674568025.985|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Timed out 1 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n",
      "%3|1674568025.985|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: 1 request(s) timed out: disconnect (after 982485ms in state UP)\n",
      "%5|1674568026.021|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Timed out MetadataRequest in flight (after 982484ms, timeout #0)\n",
      "%4|1674568026.021|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Timed out 1 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n",
      "%3|1674568026.022|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: 1 request(s) timed out: disconnect (after 982485ms in state UP)\n",
      "%4|1674568026.093|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 983991ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674568026.405|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 984444ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674568026.672|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 985243ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674568026.996|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 984568ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674568027.302|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 984465ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674568424.577|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 395495ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674568424.862|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 396223ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674568425.178|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 396096ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674568425.481|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 397431ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674568425.795|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 397806ms in state UP)\n",
      "%6|1674568426.080|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 398150ms in state UP)\n",
      "%4|1674568426.364|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 398187ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674568426.661|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 399395ms in state UP)\n",
      "%6|1674569461.250|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1032962ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674569461.463|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 1033957ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674569461.763|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 1034898ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674569462.061|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1035525ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674569462.360|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1034937ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674569462.607|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 1034885ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674569462.904|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1034301ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674569463.186|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1035870ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674569463.505|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 2421071ms in state UP, 1 identical error(s) suppressed)\n",
      "%5|1674570438.695|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Timed out MetadataRequest in flight (after 973526ms, timeout #0)\n",
      "%4|1674570438.695|REQTMOUT|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Timed out 1 in-flight, 0 retry-queued, 0 out-queue, 0 partially-sent requests\n",
      "%3|1674570438.695|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: 1 request(s) timed out: disconnect (after 973526ms in state UP)\n",
      "%4|1674570439.064|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 974747ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674570439.319|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6]: sasl_ssl://b6-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/6: Disconnected (after 975041ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674570439.600|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 975641ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674570439.892|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 976631ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674570440.215|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 975347ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674570440.496|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 976901ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674570440.785|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8]: sasl_ssl://b8-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/8: Disconnected (after 977510ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674570441.015|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 975943ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674573932.035|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 3489431ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674574873.129|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 938341ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674574873.378|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 938059ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674574873.586|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 939616ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674574873.875|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://pkc-n00kk.us-east-1.aws.confluent.cloud:9092/bootstr]: sasl_ssl://pkc-n00kk.us-east-1.aws.confluent.cloud:9092/bootstrap: Disconnected (after 939899ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674575939.108|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 1063876ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674575939.438|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1064017ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674575939.771|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1063324ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674575940.056|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1064605ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674575940.352|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1064540ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674575940.632|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://pkc-n00kk.us-east-1.aws.confluent.cloud:9092/bootstr]: sasl_ssl://pkc-n00kk.us-east-1.aws.confluent.cloud:9092/bootstrap: Disconnected (after 1064914ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674576857.936|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 915526ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674576858.206|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 916391ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674576858.489|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 917369ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674576858.754|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 917290ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674576859.013|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 917837ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674576859.313|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 916556ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674576859.646|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 916791ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674576859.976|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://pkc-n00kk.us-east-1.aws.confluent.cloud:9092/bootstr]: sasl_ssl://pkc-n00kk.us-east-1.aws.confluent.cloud:9092/bootstrap: Disconnected (after 917130ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674577813.082|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 953116ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674577813.389|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 953381ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674577813.720|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 952562ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674577813.987|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 953420ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674577814.288|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 952688ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674577814.576|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 954164ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674577814.833|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 953834ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674578880.791|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0]: sasl_ssl://b0-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/0: Disconnected (after 1065430ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674578881.009|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5]: sasl_ssl://b5-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/5: Disconnected (after 1064501ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674578881.266|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1]: sasl_ssl://b1-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/1: Disconnected (after 1065372ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674578881.511|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4]: sasl_ssl://b4-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/4: Disconnected (after 1065051ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674578881.727|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7]: sasl_ssl://b7-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/7: Disconnected (after 1067126ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674578882.029|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3]: sasl_ssl://b3-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/3: Disconnected (after 1066658ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1674578882.278|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2]: sasl_ssl://b2-pkc-n00kk.us-east-1.aws.confluent.cloud:9092/2: Disconnected (after 1066306ms in state UP, 1 identical error(s) suppressed)\n",
      "%4|1674578882.560|FAIL|rdkafka#producer-20| [thrd:sasl_ssl://pkc-n00kk.us-east-1.aws.confluent.cloud:9092/bootstr]: sasl_ssl://pkc-n00kk.us-east-1.aws.confluent.cloud:9092/bootstrap: Disconnected (after 2020793ms in state UP, 1 identical error(s) suppressed)\n"
     ]
    }
   ],
   "source": [
    "#%%capture output\n",
    "processing_col_count = 100000\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols,processing_col_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Collect your data as a pyspark dataframe and perform different operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Consider only three files for creating a dataframe among all\n",
    "case, region and TimeProvince<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all required package for step 3\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/24 23:32:47 WARN Utils: Your hostname, Aruns-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.3 instead (on interface en0)\n",
      "23/01/24 23:32:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/arunrathi/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/arunrathi/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f04f0d9d-06cb-4c5a-a4bc-764406a9e8d5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (2211ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (736ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (1694ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (3844ms)\n",
      ":: resolution report :: resolve 3080ms :: artifacts dl 8495ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f04f0d9d-06cb-4c5a-a4bc-764406a9e8d5\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (2728kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/24 23:32:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Reading Case file from MongoDB\n",
    "spark = SparkSession.builder.appName(\"sparkproject\").master('local[2]')\\\n",
    "    .config('spark.mongodb.input.uri', 'mongodb+srv://adminuser:12345@sparkproject.qsvzbji.mongodb.net/test')\\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://adminuser:12345@sparkproject.qsvzbji.mongodb.net/test\")\\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkproject</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f790cf7aac0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case file Pyspark dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the data, show it and Count the number of records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "case_df = spark.read\\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource')\\\n",
    "    .option( \"uri\", \"mongodb+srv://adminuser:12345@sparkproject.qsvzbji.mongodb.net/spark_project.case\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+--------+\n",
      "|                 _id|  case_id|           city|confirmed|group|      infection_case| latitude| longitude|province|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+--------+\n",
      "|{63cf645623dbb951...|1000003.0|        Guro-gu|     95.0| true| Guro-gu Call Center|37.508163|126.884387|   Seoul|\n",
      "|{63cf645623dbb951...|1000006.0|        Guro-gu|     41.0| true|Manmin Central Ch...|37.481059|126.894343|   Seoul|\n",
      "|{63cf645623dbb951...|1000007.0|from other city|     36.0| true|SMR Newly Planted...|        -|         -|   Seoul|\n",
      "|{63cf645623dbb951...|1000017.0|      Jongno-gu|      7.0| true|Korea Campus Crus...|37.594782|126.968022|   Seoul|\n",
      "|{63cf645623dbb951...|1000020.0|   Geumcheon-gu|      6.0| true|Geumcheon-gu rice...|        -|         -|   Seoul|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "case_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe the data with a describe function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, case_id: string, city: string, confirmed: string, infection_case: string, latitude: string, longitude: string, province: string]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. If there is any duplicate value drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df_changed = case_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_df_changed.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use limit function for showcasing a limited number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_limited_df = case_df_changed.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_limited_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+---------+-----+--------------------+----------+-----------+-----------------+\n",
      "|                 _id|  case_id|           city|confirmed|group|      infection_case|  latitude|  longitude|         province|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+----------+-----------+-----------------+\n",
      "|{63cf645623dbb951...|6000010.0|        Gumi-si|     10.0| true|    Gumi Elim Church|         -|          -| Gyeongsangbuk-do|\n",
      "|{63cf645623dbb951...|1000019.0|from other city|      1.0| true|Daejeon door-to-d...|         -|          -|            Seoul|\n",
      "|{63cf645623dbb951...|6000008.0|   Gyeongsan-si|     17.0| true|Gyeongsan Jeil Si...|  35.84819|   128.7621| Gyeongsangbuk-do|\n",
      "|{63cf645623dbb951...|1000021.0|from other city|      8.0| true|  Shincheonji Church|         -|          -|            Seoul|\n",
      "|{63cf645723dbb951...|2000021.0|              -|     63.0|false|contact with patient|         -|          -|      Gyeonggi-do|\n",
      "|{63cf645723dbb951...|1500008.0|              -|     15.0|false|     overseas inflow|         -|          -|          Daejeon|\n",
      "|{63cf645723dbb951...|6100008.0|from other city|      2.0| true|       Itaewon Clubs|         -|          -| Gyeongsangnam-do|\n",
      "|{63cf645723dbb951...|2000012.0|       Suwon-si|     15.0| true|Lotte Confectione...| 37.287356| 127.013827|      Gyeonggi-do|\n",
      "|{63cf645723dbb951...|2000010.0|    Seongnam-si|     22.0| true|Bundang Jesaeng H...|  37.38833|   127.1218|      Gyeonggi-do|\n",
      "|{63cf645623dbb951...|1500004.0|         Seo-gu|      4.0| true|     Dreaming Church| 36.346869| 127.368594|          Daejeon|\n",
      "|{63cf645623dbb951...|1000038.0|              -|    100.0|false|                 etc|         -|          -|            Seoul|\n",
      "|{63cf645623dbb951...|6000003.0|    Bonghwa-gun|     68.0| true|Bonghwa Pureun Nu...|  36.92757|   128.9099| Gyeongsangbuk-do|\n",
      "|{63cf645723dbb951...|1200003.0|         Seo-gu|    124.0| true|Hansarang Convale...| 35.885592| 128.556649|            Daegu|\n",
      "|{63cf645723dbb951...|2000001.0|    Seongnam-si|     67.0| true|River of Grace Co...| 37.455687| 127.161627|      Gyeonggi-do|\n",
      "|{63cf645623dbb951...|4100003.0|      Seosan-si|      9.0| true|Seosan-si Laboratory| 37.000354| 126.354443|Chungcheongnam-do|\n",
      "|{63cf645723dbb951...|1500003.0|         Seo-gu|      7.0| true|         Orange Town|36.3398739|127.3819744|          Daejeon|\n",
      "|{63cf645723dbb951...|6000011.0|              -|     22.0|false|     overseas inflow|         -|          -| Gyeongsangbuk-do|\n",
      "|{63cf645723dbb951...|1000018.0|     Gangnam-gu|      6.0| true|Gangnam Yeoksam-d...|         -|          -|            Seoul|\n",
      "|{63cf645623dbb951...|1000006.0|        Guro-gu|     41.0| true|Manmin Central Ch...| 37.481059| 126.894343|            Seoul|\n",
      "|{63cf645623dbb951...|1000017.0|      Jongno-gu|      7.0| true|Korea Campus Crus...| 37.594782| 126.968022|            Seoul|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+----------+-----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "case_limited_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Select the subset of the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------+\n",
      "|           city|        province|confirmed|\n",
      "+---------------+----------------+---------+\n",
      "|        Gumi-si|Gyeongsangbuk-do|     10.0|\n",
      "|from other city|           Seoul|      1.0|\n",
      "|   Gyeongsan-si|Gyeongsangbuk-do|     17.0|\n",
      "|from other city|           Seoul|      8.0|\n",
      "|              -|     Gyeonggi-do|     63.0|\n",
      "+---------------+----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "case_selected_cols_df = case_df_changed.select('city', 'province', 'confirmed')\n",
    "case_selected_cols_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. If there is any null value, fill it with any random value or drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing / null values have been replaced with values mentioned below:\n",
    "# For int type null replaced with -99999\n",
    "# For float type null replaced with -99999.99\n",
    "# for string type null replaced with *miss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace_df = case_df_changed.select([when(col(c)==99999,0).otherwise(col(c)).alias(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_null_df = case_df_changed.na.fill(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Filter the data based on different columns or variables and do the best analysis.\n",
    "<br>For example: We can filter a data frame using multiple\n",
    "conditions using AND(&), OR(|) and NOT(~) conditions. For\n",
    "example, we may want to find out all the dif erent\n",
    "infection_case in Daegu Province with more than 10\n",
    "confirmed cases.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+---------+-----+--------------------+--------+---------+----------------+\n",
      "|                 _id|  case_id|           city|confirmed|group|      infection_case|latitude|longitude|        province|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+--------+---------+----------------+\n",
      "|{63cf645623dbb951...|6000010.0|        Gumi-si|     10.0| true|    Gumi Elim Church|       -|        -|Gyeongsangbuk-do|\n",
      "|{63cf645623dbb951...|1000019.0|from other city|      1.0| true|Daejeon door-to-d...|       -|        -|           Seoul|\n",
      "|{63cf645623dbb951...|6000008.0|   Gyeongsan-si|     17.0| true|Gyeongsan Jeil Si...|35.84819| 128.7621|Gyeongsangbuk-do|\n",
      "|{63cf645623dbb951...|1000021.0|from other city|      8.0| true|  Shincheonji Church|       -|        -|           Seoul|\n",
      "|{63cf645723dbb951...|2000021.0|              -|     63.0|false|contact with patient|       -|        -|     Gyeonggi-do|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+--------+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df_changed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = case_df_changed.filter((col(\"province\") == \"Daegu\") & (col(\"confirmed\") > 10)).select(\"infection_case\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      infection_case|\n",
      "+--------------------+\n",
      "|     overseas inflow|\n",
      "|Hansarang Convale...|\n",
      "|Second Mi-Ju Hosp...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Sort the number of confirmed cases. Confirmed column is there in the dataset. Check with descending sort also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "|                 _id|  case_id|           city|confirmed|group|      infection_case| latitude| longitude|        province|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "|{63cf645723dbb951...|1200001.0|         Nam-gu|   4511.0| true|  Shincheonji Church| 35.84008|  128.5667|           Daegu|\n",
      "|{63cf645623dbb951...|1200009.0|              -|    917.0|false|contact with patient|        -|         -|           Daegu|\n",
      "|{63cf645723dbb951...|1200010.0|              -|    747.0|false|                 etc|        -|         -|           Daegu|\n",
      "|{63cf645623dbb951...|6000001.0|from other city|    566.0| true|  Shincheonji Church|        -|         -|Gyeongsangbuk-do|\n",
      "|{63cf645723dbb951...|2000020.0|              -|    305.0|false|     overseas inflow|        -|         -|     Gyeonggi-do|\n",
      "|{63cf645623dbb951...|1000036.0|              -|    298.0|false|     overseas inflow|        -|         -|           Seoul|\n",
      "|{63cf645623dbb951...|1200002.0|   Dalseong-gun|    196.0| true|Second Mi-Ju Hosp...|35.857375|128.466651|           Daegu|\n",
      "|{63cf645623dbb951...|6000012.0|              -|    190.0|false|contact with patient|        -|         -|Gyeongsangbuk-do|\n",
      "|{63cf645723dbb951...|1000037.0|              -|    162.0|false|contact with patient|        -|         -|           Seoul|\n",
      "|{63cf645723dbb951...|1000001.0|     Yongsan-gu|    139.0| true|       Itaewon Clubs|37.538621|126.992652|           Seoul|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df_changed.sort(col(\"confirmed\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "|                 _id|  case_id|           city|confirmed|group|      infection_case| latitude| longitude|        province|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "|{63cf645723dbb951...|1200001.0|         Nam-gu|   4511.0| true|  Shincheonji Church| 35.84008|  128.5667|           Daegu|\n",
      "|{63cf645623dbb951...|1200009.0|              -|    917.0|false|contact with patient|        -|         -|           Daegu|\n",
      "|{63cf645723dbb951...|1200010.0|              -|    747.0|false|                 etc|        -|         -|           Daegu|\n",
      "|{63cf645623dbb951...|6000001.0|from other city|    566.0| true|  Shincheonji Church|        -|         -|Gyeongsangbuk-do|\n",
      "|{63cf645723dbb951...|2000020.0|              -|    305.0|false|     overseas inflow|        -|         -|     Gyeonggi-do|\n",
      "|{63cf645623dbb951...|1000036.0|              -|    298.0|false|     overseas inflow|        -|         -|           Seoul|\n",
      "|{63cf645623dbb951...|1200002.0|   Dalseong-gun|    196.0| true|Second Mi-Ju Hosp...|35.857375|128.466651|           Daegu|\n",
      "|{63cf645623dbb951...|6000012.0|              -|    190.0|false|contact with patient|        -|         -|Gyeongsangbuk-do|\n",
      "|{63cf645723dbb951...|1000037.0|              -|    162.0|false|contact with patient|        -|         -|           Seoul|\n",
      "|{63cf645723dbb951...|1000001.0|     Yongsan-gu|    139.0| true|       Itaewon Clubs|37.538621|126.992652|           Seoul|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df_changed.orderBy(col(\"confirmed\").desc()).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. In case of any wrong data type, cast that data type from integer to string or string to integer.\n",
    "Use group by on top of province and city column and agg it\n",
    "with sum of confirmed cases. For example\n",
    "df.groupBy([\"province\",\"city\"]).agg(function.sum(\"co\n",
    "nfirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- case_id: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- confirmed: double (nullable = true)\n",
      " |-- group: boolean (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df_changed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- confirmed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datatype_changed_df = case_df_changed.select(col(\"case_id\"), col(\"city\"), col(\"latitude\").cast(DoubleType()).alias(\"latitude\"),\\\n",
    "    col(\"longitude\").cast(DoubleType()).alias(\"longitude\"), col(\"confirmed\"))\n",
    "\n",
    "datatype_changed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------+---------+---------+\n",
      "|  case_id|           city|latitude|longitude|confirmed|\n",
      "+---------+---------------+--------+---------+---------+\n",
      "|6000010.0|        Gumi-si|    null|     null|     10.0|\n",
      "|1000019.0|from other city|    null|     null|      1.0|\n",
      "|6000008.0|   Gyeongsan-si|35.84819| 128.7621|     17.0|\n",
      "|1000021.0|from other city|    null|     null|      8.0|\n",
      "|2000021.0|              -|    null|     null|     63.0|\n",
      "+---------+---------------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datatype_changed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------+---------+---------+\n",
      "|  case_id|           city|latitude|longitude|confirmed|\n",
      "+---------+---------------+--------+---------+---------+\n",
      "|6000010.0|        Gumi-si|     0.0|      0.0|     10.0|\n",
      "|1000019.0|from other city|     0.0|      0.0|      1.0|\n",
      "|6000008.0|   Gyeongsan-si|35.84819| 128.7621|     17.0|\n",
      "|1000021.0|from other city|     0.0|      0.0|      8.0|\n",
      "|2000021.0|              -|     0.0|      0.0|     63.0|\n",
      "+---------+---------------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datatype_changed_df.na.fill(0).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region file Pyspark dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the data, show it and Count the number of records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "region_df = spark.read\\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource')\\\n",
    "    .option( \"uri\", \"mongodb+srv://adminuser:12345@sparkproject.qsvzbji.mongodb.net/spark_project.region\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------+-----+-------------------+------------------------+-----------------------+------------------+---------+----------+------------------+--------+----------------+\n",
      "|                 _id|academy_ratio|         city| code|elderly_alone_ratio|elderly_population_ratio|elementary_school_count|kindergarten_count| latitude| longitude|nursing_home_count|province|university_count|\n",
      "+--------------------+-------------+-------------+-----+-------------------+------------------------+-----------------------+------------------+---------+----------+------------------+--------+----------------+\n",
      "|{63cf6cca23dbb951...|         1.06|Dongdaemun-gu|10110|                6.7|                   17.26|                     21|                31|37.574552|127.039721|               832|   Seoul|               4|\n",
      "|{63cf6cca23dbb951...|         0.96| Geumcheon-gu|10080|                6.7|                   16.15|                     18|                19|37.456852|126.895229|               475|   Seoul|               0|\n",
      "|{63cf6cca23dbb951...|          2.6|    Seocho-gu|10150|                3.8|                   13.39|                     24|                27|37.483804|127.032693|              1465|   Seoul|               1|\n",
      "|{63cf6cca23dbb951...|         2.26| Yangcheon-gu|10190|                5.5|                   13.55|                     30|                43|37.517189|126.866618|               816|   Seoul|               0|\n",
      "|{63cf6cca23dbb951...|         0.94|      Jung-gu|10240|                7.4|                   18.42|                     12|                14|37.563988| 126.99753|               728|   Seoul|               2|\n",
      "+--------------------+-------------+-------------+-----+-------------------+------------------------+-----------------------+------------------+---------+----------+------------------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "region_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe the data with a describe function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, academy_ratio: string, city: string, code: string, elderly_alone_ratio: string, elderly_population_ratio: string, elementary_school_count: string, kindergarten_count: string, latitude: string, longitude: string, nursing_home_count: string, province: string, university_count: string]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. If there is any duplicate value drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df_changed = region_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df_changed.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use limit function for showcasing a limited number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_limited_df = region_df_changed.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_limited_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Select the subset of the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-------------------+------------------------+-----------------------+\n",
      "|        city| code|elderly_alone_ratio|elderly_population_ratio|elementary_school_count|\n",
      "+------------+-----+-------------------+------------------------+-----------------------+\n",
      "| Seongnam-si|20120|                5.6|                   13.52|                     72|\n",
      "|    Gimpo-si|20080|                4.4|                    12.1|                     43|\n",
      "|   Andong-si|60110|               12.4|                   23.95|                     30|\n",
      "|Tongyeong-si|61140|                9.8|                   18.47|                     20|\n",
      "|Geumcheon-gu|10080|                6.7|                   16.15|                     18|\n",
      "+------------+-----+-------------------+------------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "region_selected_cols_df = region_df_changed.select(\"city\",\"code\", \"elderly_alone_ratio\", \"elderly_population_ratio\", \"elementary_school_count\")\n",
    "region_selected_cols_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. If there is any null value, fill it with any random value or drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing / null values have been replaced with values mentioned below:\n",
    "# For int type null replaced with -99999\n",
    "# For float type null replaced with -99999.99\n",
    "# for string type null replaced with *miss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace_df = case_df_changed.select([when(col(c)==99999,0).otherwise(col(c)).alias(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_null_df = region_df_changed.na.fill(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Filter the data based on different columns or variables and do the best analysis.\n",
    "<br>For example: We can filter a data frame using multiple\n",
    "conditions using AND(&), OR(|) and NOT(~) conditions. For\n",
    "example, we may want to find out all the dif erent\n",
    "infection_case in Daegu Province with more than 10\n",
    "confirmed cases.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------------+------------------------+-----------------------+\n",
      "|       city| code|elderly_alone_ratio|elderly_population_ratio|elementary_school_count|\n",
      "+-----------+-----+-------------------+------------------------+-----------------------+\n",
      "|Seongnam-si|20120|                5.6|                   13.52|                     72|\n",
      "+-----------+-----+-------------------+------------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "region_selected_cols_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = region_df_changed.filter((col(\"province\") == \"Daegu\")).select(\"elementary_school_count\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|elementary_school_count|\n",
      "+-----------------------+\n",
      "|                     34|\n",
      "|                    229|\n",
      "|                     32|\n",
      "+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Sort the number of confirmed cases. Confirmed column is there in the dataset. Check with descending sort also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------------+\n",
      "|             city|         province|elementary_school_count|\n",
      "+-----------------+-----------------+-----------------------+\n",
      "|            Korea|            Korea|                   6087|\n",
      "|      Gyeonggi-do|      Gyeonggi-do|                   1277|\n",
      "|            Seoul|            Seoul|                    607|\n",
      "| Gyeongsangnam-do| Gyeongsangnam-do|                    501|\n",
      "| Gyeongsangbuk-do| Gyeongsangbuk-do|                    471|\n",
      "|     Jeollanam-do|     Jeollanam-do|                    429|\n",
      "|     Jeollabuk-do|     Jeollabuk-do|                    419|\n",
      "|Chungcheongnam-do|Chungcheongnam-do|                    409|\n",
      "|       Gangwon-do|       Gangwon-do|                    349|\n",
      "|            Busan|            Busan|                    304|\n",
      "+-----------------+-----------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "region_df_changed.sort(col(\"elementary_school_count\").desc()).select(\"city\", \"province\", \"elementary_school_count\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------------+\n",
      "|             city|         province|elementary_school_count|\n",
      "+-----------------+-----------------+-----------------------+\n",
      "|            Korea|            Korea|                   6087|\n",
      "|      Gyeonggi-do|      Gyeonggi-do|                   1277|\n",
      "|            Seoul|            Seoul|                    607|\n",
      "| Gyeongsangnam-do| Gyeongsangnam-do|                    501|\n",
      "| Gyeongsangbuk-do| Gyeongsangbuk-do|                    471|\n",
      "|     Jeollanam-do|     Jeollanam-do|                    429|\n",
      "|     Jeollabuk-do|     Jeollabuk-do|                    419|\n",
      "|Chungcheongnam-do|Chungcheongnam-do|                    409|\n",
      "|       Gangwon-do|       Gangwon-do|                    349|\n",
      "|            Busan|            Busan|                    304|\n",
      "+-----------------+-----------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "region_df_changed.orderBy(col(\"elementary_school_count\").desc()).select(\"city\", \"province\", \"elementary_school_count\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. In case of any wrong data type, cast that data type from integer to string or string to integer.\n",
    "Use group by on top of province and city column and agg it\n",
    "with sum of confirmed cases. For example\n",
    "df.groupBy([\"province\",\"city\"]).agg(function.sum(\"co\n",
    "nfirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- academy_ratio: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- code: long (nullable = true)\n",
      " |-- elderly_alone_ratio: double (nullable = true)\n",
      " |-- elderly_population_ratio: double (nullable = true)\n",
      " |-- elementary_school_count: long (nullable = true)\n",
      " |-- kindergarten_count: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- nursing_home_count: long (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- university_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_df_changed.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeProvince file Pyspark dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the data, show it and Count the number of records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timeprovince_df = spark.read\\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource')\\\n",
    "    .option( \"uri\", \"mongodb+srv://adminuser:12345@sparkproject.qsvzbji.mongodb.net/spark_project.timeprovince\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+--------+----------------+--------+----+\n",
      "|                 _id|confirmed|      date|deceased|        province|released|time|\n",
      "+--------------------+---------+----------+--------+----------------+--------+----+\n",
      "|{63cf830923dbb951...|        1|2020-01-20|       0|         Incheon|       0|  16|\n",
      "|{63cf830923dbb951...|        0|2020-01-23|       0|    Jeollabuk-do|       0|  16|\n",
      "|{63cf830923dbb951...|        0|2020-02-08|       0|          Sejong|       0|  16|\n",
      "|{63cf830923dbb951...|        0|2020-02-10|       0|      Gangwon-do|       0|  16|\n",
      "|{63cf830923dbb951...|        0|2020-02-11|       0|Gyeongsangbuk-do|       1|  16|\n",
      "+--------------------+---------+----------+--------+----------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeprovince_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2771"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeprovince_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe the data with a describe function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, confirmed: string, date: string, deceased: string, province: string, released: string, time: string]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeprovince_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. If there is any duplicate value drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeprovince_df_changed = timeprovince_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2771"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeprovince_df_changed.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use limit function for showcasing a limited number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeprovince_limited_df = timeprovince_df_changed.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeprovince_limited_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Select the subset of the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+--------+--------+--------+----+\n",
      "|                 _id|confirmed|      date|deceased|province|released|time|\n",
      "+--------------------+---------+----------+--------+--------+--------+----+\n",
      "|{63cf830923dbb951...|        1|2020-02-23|       0|   Ulsan|       0|  16|\n",
      "|{63cf830923dbb951...|     6031|2020-03-15|      53|   Daegu|     468|   0|\n",
      "+--------------------+---------+----------+--------+--------+--------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timeprovince_df_changed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+--------+\n",
      "|      date|   province|confirmed|released|\n",
      "+----------+-----------+---------+--------+\n",
      "|2020-02-23|      Ulsan|        1|       0|\n",
      "|2020-03-15|      Daegu|     6031|     468|\n",
      "|2020-03-30|      Seoul|      426|      92|\n",
      "|2020-04-05|    Incheon|       79|      25|\n",
      "|2020-02-08|Gyeonggi-do|       10|       0|\n",
      "+----------+-----------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timeprovince_selected_cols_df = timeprovince_df_changed.select(\"date\",\"province\", \"confirmed\", \"released\")\n",
    "timeprovince_selected_cols_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. If there is any null value, fill it with any random value or drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing / null values have been replaced with values mentioned below:\n",
    "# For int type null replaced with -99999\n",
    "# For float type null replaced with -99999.99\n",
    "# for string type null replaced with *miss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace_df = case_df_changed.select([when(col(c)==99999,0).otherwise(col(c)).alias(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_null_df = timeprovince_df_changed.na.fill(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Filter the data based on different columns or variables and do the best analysis.\n",
    "<br>For example: We can filter a data frame using multiple\n",
    "conditions using AND(&), OR(|) and NOT(~) conditions. For\n",
    "example, we may want to find out all the dif erent\n",
    "infection_case in Daegu Province with more than 10\n",
    "confirmed cases.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+--------+\n",
      "|      date|province|confirmed|released|\n",
      "+----------+--------+---------+--------+\n",
      "|2020-02-23|   Ulsan|        1|       0|\n",
      "+----------+--------+---------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timeprovince_selected_cols_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = timeprovince_df_changed.filter((col(\"confirmed\") > 10) & (col(\"date\") > \"2020-01-31\")).select(\"province\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 131:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         province|\n",
      "+-----------------+\n",
      "|           Sejong|\n",
      "|            Ulsan|\n",
      "|Chungcheongbuk-do|\n",
      "+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_df.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Sort the number of confirmed cases. Confirmed column is there in the dataset. Check with descending sort also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+--------+\n",
      "|      date|province|confirmed|released|\n",
      "+----------+--------+---------+--------+\n",
      "|2020-06-29|   Daegu|     6906|    6700|\n",
      "|2020-06-30|   Daegu|     6906|    6700|\n",
      "|2020-06-27|   Daegu|     6904|    6700|\n",
      "|2020-06-28|   Daegu|     6904|    6700|\n",
      "|2020-06-24|   Daegu|     6903|    6687|\n",
      "|2020-06-25|   Daegu|     6903|    6689|\n",
      "|2020-06-26|   Daegu|     6903|    6695|\n",
      "|2020-06-23|   Daegu|     6901|    6686|\n",
      "|2020-06-22|   Daegu|     6900|    6681|\n",
      "|2020-06-21|   Daegu|     6899|    6680|\n",
      "+----------+--------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timeprovince_df_changed.sort(col(\"confirmed\").desc()).select(\"date\",\"province\", \"confirmed\", \"released\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+--------+\n",
      "|      date|province|confirmed|released|\n",
      "+----------+--------+---------+--------+\n",
      "|2020-06-29|   Daegu|     6906|    6700|\n",
      "|2020-06-30|   Daegu|     6906|    6700|\n",
      "|2020-06-27|   Daegu|     6904|    6700|\n",
      "|2020-06-28|   Daegu|     6904|    6700|\n",
      "|2020-06-24|   Daegu|     6903|    6687|\n",
      "|2020-06-25|   Daegu|     6903|    6689|\n",
      "|2020-06-26|   Daegu|     6903|    6695|\n",
      "|2020-06-23|   Daegu|     6901|    6686|\n",
      "|2020-06-22|   Daegu|     6900|    6681|\n",
      "|2020-06-21|   Daegu|     6899|    6680|\n",
      "+----------+--------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "timeprovince_df_changed.orderBy(col(\"confirmed\").desc()).select(\"date\",\"province\", \"confirmed\", \"released\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. In case of any wrong data type, cast that data type from integer to string or string to integer.\n",
    "Use group by on top of province and city column and agg it\n",
    "with sum of confirmed cases. For example\n",
    "df.groupBy([\"province\",\"city\"]).agg(function.sum(\"co\n",
    "nfirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- confirmed: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- deceased: long (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- released: long (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeprovince_df_changed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- confirmed: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datatype_changed_df = timeprovince_df_changed.select(col(\"date\").cast(DateType()).alias(\"Date\"),\\\n",
    "    col(\"province\"), col(\"confirmed\"))\n",
    "\n",
    "datatype_changed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 140:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+\n",
      "|      Date|   province|confirmed|\n",
      "+----------+-----------+---------+\n",
      "|2020-02-23|      Ulsan|        1|\n",
      "|2020-03-15|      Daegu|     6031|\n",
      "|2020-03-30|      Seoul|      426|\n",
      "|2020-04-05|    Incheon|       79|\n",
      "|2020-02-08|Gyeonggi-do|       10|\n",
      "+----------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "datatype_changed_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. For joins we will need one more file.you can use region file.\n",
    "User different different join methods.for example\n",
    "cases.join(regions, ['province','city'],how='left')\n",
    "You can do your best analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+---------+-----+--------------------+--------+---------+----------------+\n",
      "|                 _id|  case_id|           city|confirmed|group|      infection_case|latitude|longitude|        province|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+--------+---------+----------------+\n",
      "|{63cf645623dbb951...|6000010.0|        Gumi-si|     10.0| true|    Gumi Elim Church|       -|        -|Gyeongsangbuk-do|\n",
      "|{63cf645623dbb951...|1000019.0|from other city|      1.0| true|Daejeon door-to-d...|       -|        -|           Seoul|\n",
      "|{63cf645623dbb951...|6000008.0|   Gyeongsan-si|     17.0| true|Gyeongsan Jeil Si...|35.84819| 128.7621|Gyeongsangbuk-do|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+--------+---------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "case_df_changed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 149:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+-----+-------------------+------------------------+-----------------------+------------------+---------+----------+------------------+----------------+----------------+\n",
      "|                 _id|academy_ratio|       city| code|elderly_alone_ratio|elderly_population_ratio|elementary_school_count|kindergarten_count| latitude| longitude|nursing_home_count|        province|university_count|\n",
      "+--------------------+-------------+-----------+-----+-------------------+------------------------+-----------------------+------------------+---------+----------+------------------+----------------+----------------+\n",
      "|{63cf6ccc23dbb951...|         1.74|   Gimpo-si|20080|                4.4|                    12.1|                     43|                90|37.615238|126.715601|               604|     Gyeonggi-do|               2|\n",
      "|{63cf6ccc23dbb951...|         2.08|Seongnam-si|20120|                5.6|                   13.52|                     72|               127|    37.42|127.126703|              2095|     Gyeonggi-do|               3|\n",
      "|{63cf6ccb23dbb951...|         1.57|  Andong-si|60110|               12.4|                   23.95|                     30|                42|36.568441|128.729551|               290|Gyeongsangbuk-do|               3|\n",
      "+--------------------+-------------+-----------+-----+-------------------+------------------------+-----------------------+------------------+---------+----------+------------------+----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "region_df_changed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+--------+-------------------+------------------------+-----------------------+------------------+\n",
      "|   province|                 _id|  case_id|           city|confirmed|group|      infection_case| latitude| longitude|    city|elderly_alone_ratio|elderly_population_ratio|elementary_school_count|kindergarten_count|\n",
      "+-----------+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+--------+-------------------+------------------------+-----------------------+------------------+\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000018.0|from other city|      5.0| true|Seoul City Hall S...|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000003.0|from other city|     59.0| true|       Itaewon Clubs|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000013.0|      Anyang-si|     17.0| true|   Lord Glory Church|37.403722|126.954939|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000002.0|     Bucheon-si|     67.0| true|Coupang Logistics...|37.530579|126.775254|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000017.0|from other city|      6.0| true|     Wangsung Church|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000022.0|              -|     84.0|false|                 etc|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000011.0|      Anyang-si|     22.0| true|Anyang Gunpo Past...|37.381784| 126.93615|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000005.0|   Uijeongbu-si|     50.0| true|Uijeongbu St. Mar...|37.758635|127.077716|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000004.0|from other city|     58.0| true|             Richway|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645723dbb951...|2000008.0|from other city|     28.0| true|Yangcheon Table T...|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000014.0|       Suwon-si|     10.0| true|Suwon Saeng Myeon...|  37.2376|  127.0517|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000009.0|              -|     25.0| true|SMR Newly Planted...|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000016.0|from other city|      6.0| true|Geumcheon-gu rice...|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000015.0|from other city|      7.0| true|Korea Campus Crus...|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000007.0|from other city|     29.0| true|  Shincheonji Church|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645723dbb951...|2000020.0|              -|    305.0|false|     overseas inflow|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645623dbb951...|2000019.0|    Seongnam-si|      5.0| true|Seongnam neighbor...|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645723dbb951...|2000006.0|from other city|     50.0| true| Guro-gu Call Center|        -|         -|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645723dbb951...|2000001.0|    Seongnam-si|     67.0| true|River of Grace Co...|37.455687|127.161627|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "|Gyeonggi-do|{63cf645723dbb951...|2000010.0|    Seongnam-si|     22.0| true|Bundang Jesaeng H...| 37.38833|  127.1218|Gimpo-si|                4.4|                    12.1|                     43|                90|\n",
      "+-----------+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+--------+-------------------+------------------------+-----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "case_df_changed.join(region_df_changed.select(\"province\", \"city\", \"elderly_alone_ratio\",\\\n",
    "    \"elderly_population_ratio\", \"elementary_school_count\",\"kindergarten_count\"), ['province'], how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 177:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+---------+---------+-----+--------------------+---------+----------+-------------------+------------------------+-----------------------+\n",
      "|        province|        city|  case_id|confirmed|group|      infection_case| latitude| longitude|elderly_alone_ratio|elderly_population_ratio|elementary_school_count|\n",
      "+----------------+------------+---------+---------+-----+--------------------+---------+----------+-------------------+------------------------+-----------------------+\n",
      "|     Gyeonggi-do| Seongnam-si|2000019.0|      5.0| true|Seongnam neighbor...|        -|         -|                5.6|                   13.52|                     72|\n",
      "|     Gyeonggi-do| Seongnam-si|2000001.0|     67.0| true|River of Grace Co...|37.455687|127.161627|                5.6|                   13.52|                     72|\n",
      "|     Gyeonggi-do| Seongnam-si|2000010.0|     22.0| true|Bundang Jesaeng H...| 37.38833|  127.1218|                5.6|                   13.52|                     72|\n",
      "|           Seoul|Geumcheon-gu|1000020.0|      6.0| true|Geumcheon-gu rice...|        -|         -|                6.7|                   16.15|                     18|\n",
      "|           Busan|  Dongnae-gu|1100001.0|     39.0| true|       Onchun Church| 35.21628|  129.0771|                7.7|                   17.53|                     22|\n",
      "|           Seoul|     Jung-gu|1000032.0|      3.0| true|Seoul City Hall S...|37.565699|126.977079|                7.4|                   18.42|                     12|\n",
      "|           Seoul|     Jung-gu|1000023.0|     13.0| true|   KB Life Insurance|37.560899|126.966998|                7.4|                   18.42|                     12|\n",
      "|           Seoul|     Jung-gu|1000015.0|      7.0| true|Jung-gu Fashion C...|37.562405|126.984377|                7.4|                   18.42|                     12|\n",
      "|           Daegu|      Nam-gu|1200001.0|   4511.0| true|  Shincheonji Church| 35.84008|  128.5667|               10.4|                   22.49|                     11|\n",
      "|           Daegu|Dalseong-gun|1200004.0|    101.0| true|Daesil Convalesce...|35.857393|128.466653|                5.4|                   12.11|                     32|\n",
      "|           Daegu|Dalseong-gun|1200002.0|    196.0| true|Second Mi-Ju Hosp...|35.857375|128.466651|                5.4|                   12.11|                     32|\n",
      "|           Seoul|   Gwanak-gu|1000002.0|    119.0| true|             Richway| 37.48208|126.901384|                4.9|                   15.12|                     22|\n",
      "|           Seoul|   Gwanak-gu|1000010.0|     30.0| true|     Wangsung Church|37.481735|126.930121|                4.9|                   15.12|                     22|\n",
      "|Gyeongsangbuk-do| Bonghwa-gun|6000003.0|     68.0| true|Bonghwa Pureun Nu...| 36.92757|  128.9099|               20.0|                   35.26|                     14|\n",
      "|Gyeongsangnam-do|  Yangsan-si|6100007.0|      3.0| true|         Soso Seowon|35.338811|129.017508|                6.2|                   12.92|                     37|\n",
      "|Gyeongsangbuk-do|Cheongdo-gun|6000002.0|    119.0| true|Cheongdo Daenam H...| 35.64887|  128.7368|               21.0|                   36.55|                     11|\n",
      "|           Seoul|   Seocho-gu|1000027.0|      5.0| true|       Seocho Family|        -|         -|                3.8|                   13.39|                     24|\n",
      "|           Seoul|Yangcheon-gu|1000004.0|     43.0| true|Yangcheon Table T...|37.546061|126.874209|                5.5|                   13.55|                     30|\n",
      "|           Seoul|Yangcheon-gu|1000026.0|      3.0| true|Biblical Language...|37.524623|126.843118|                5.5|                   13.55|                     30|\n",
      "|Gyeongsangbuk-do| Chilgok-gun|6000007.0|     36.0| true|       Milal Shelter|  36.0581|  128.4941|                6.7|                   15.17|                     21|\n",
      "+----------------+------------+---------+---------+-----+--------------------+---------+----------+-------------------+------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "case_df_changed.drop('_id').join(region_df_changed.select(\"province\", \"city\", \"elderly_alone_ratio\",\\\n",
    "    \"elderly_population_ratio\", \"elementary_school_count\"), ['province', 'city'], how='inner').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - If you want, you can also use SQL with data frames. Let us try to run some SQL on the cases table.\n",
    "For example:<br>\n",
    "cases.registerTempTable('cases_table')<br>\n",
    "newDF = sqlContext.sql('select * from cases_table where\n",
    "confirmed>100')<br>\n",
    "newDF.show()\n",
    "<br>\n",
    "<t>\n",
    "<br>\n",
    "Here is a example how you can use df for sql now you can perform\n",
    "various operations with GROUP BY, HAVING, AND ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "|                 _id|  case_id|           city|confirmed|group|      infection_case| latitude| longitude|        province|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "|{63cf645623dbb951...|6000010.0|        Gumi-si|     10.0| true|    Gumi Elim Church|        -|         -|Gyeongsangbuk-do|\n",
      "|{63cf645623dbb951...|1000019.0|from other city|      1.0| true|Daejeon door-to-d...|        -|         -|           Seoul|\n",
      "|{63cf645623dbb951...|6000008.0|   Gyeongsan-si|     17.0| true|Gyeongsan Jeil Si...| 35.84819|  128.7621|Gyeongsangbuk-do|\n",
      "|{63cf645623dbb951...|1000021.0|from other city|      8.0| true|  Shincheonji Church|        -|         -|           Seoul|\n",
      "|{63cf645723dbb951...|2000021.0|              -|     63.0|false|contact with patient|        -|         -|     Gyeonggi-do|\n",
      "|{63cf645723dbb951...|1500008.0|              -|     15.0|false|     overseas inflow|        -|         -|         Daejeon|\n",
      "|{63cf645723dbb951...|6100008.0|from other city|      2.0| true|       Itaewon Clubs|        -|         -|Gyeongsangnam-do|\n",
      "|{63cf645723dbb951...|2000012.0|       Suwon-si|     15.0| true|Lotte Confectione...|37.287356|127.013827|     Gyeonggi-do|\n",
      "|{63cf645723dbb951...|2000010.0|    Seongnam-si|     22.0| true|Bundang Jesaeng H...| 37.38833|  127.1218|     Gyeonggi-do|\n",
      "|{63cf645623dbb951...|1500004.0|         Seo-gu|      4.0| true|     Dreaming Church|36.346869|127.368594|         Daejeon|\n",
      "+--------------------+---------+---------------+---------+-----+--------------------+---------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case_df_changed.createOrReplaceTempView(\"cases_table\")\n",
    "case_table_dF = spark.sql('select * from cases_table limit 10')\n",
    "case_table_dF.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Create Spark UDFs\n",
    "Create function casehighlow()<br>\n",
    "If case is less than 50 return low else return high<br>\n",
    "convert into a UDF Function and mention the return type of\n",
    "function.<br>\n",
    "Note: You can create as many as udf based on analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def casehighlow(case):\n",
    "    ans = \"\"\n",
    "    if case < 50:\n",
    "        ans = \"low\"\n",
    "    else:\n",
    "        ans = \"high\"\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------------+---------+----+\n",
      "|  case_id|           city|         province|confirmed|Type|\n",
      "+---------+---------------+-----------------+---------+----+\n",
      "|6000010.0|        Gumi-si| Gyeongsangbuk-do|     10.0| low|\n",
      "|1000019.0|from other city|            Seoul|      1.0| low|\n",
      "|6000008.0|   Gyeongsan-si| Gyeongsangbuk-do|     17.0| low|\n",
      "|1000021.0|from other city|            Seoul|      8.0| low|\n",
      "|2000021.0|              -|      Gyeonggi-do|     63.0|high|\n",
      "|1500008.0|              -|          Daejeon|     15.0| low|\n",
      "|6100008.0|from other city| Gyeongsangnam-do|      2.0| low|\n",
      "|2000012.0|       Suwon-si|      Gyeonggi-do|     15.0| low|\n",
      "|2000010.0|    Seongnam-si|      Gyeonggi-do|     22.0| low|\n",
      "|1500004.0|         Seo-gu|          Daejeon|      4.0| low|\n",
      "|1000038.0|              -|            Seoul|    100.0|high|\n",
      "|6000003.0|    Bonghwa-gun| Gyeongsangbuk-do|     68.0|high|\n",
      "|1200003.0|         Seo-gu|            Daegu|    124.0|high|\n",
      "|2000001.0|    Seongnam-si|      Gyeonggi-do|     67.0|high|\n",
      "|4100003.0|      Seosan-si|Chungcheongnam-do|      9.0| low|\n",
      "|1500003.0|         Seo-gu|          Daejeon|      7.0| low|\n",
      "|6000011.0|              -| Gyeongsangbuk-do|     22.0| low|\n",
      "|1000018.0|     Gangnam-gu|            Seoul|      6.0| low|\n",
      "|1000006.0|        Guro-gu|            Seoul|     41.0| low|\n",
      "|1000017.0|      Jongno-gu|            Seoul|      7.0| low|\n",
      "+---------+---------------+-----------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "udf_df = case_df_changed.select(\"case_id\", \"city\", \"province\", \"confirmed\")\\\n",
    "    .withColumn('Type', casehighlow(col(\"confirmed\")))\n",
    "udf_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
