{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Project\n",
    "\n",
    "**Dataset** - https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset\n",
    "\n",
    "Step 1 - Create a producer with a python connector in confluent kafka and\n",
    "stream your data.\n",
    "\n",
    "Step 2 - Consume your data through the python connector and dump it in\n",
    "mongodb atlas.\n",
    "Note: Here in the dataset you will be finding a multiple files you\n",
    "need to use all file for the kafka and mongodb\n",
    "\n",
    "Step 3 - Collect your data as a pyspark dataframe and perform different\n",
    "operations.<br>\n",
    "Note: Consider only three files for creating a dataframe among all\n",
    "case, region and TimeProvince<br>\n",
    "1. Read the data, show it and Count the number of records.\n",
    "2. Describe the data with a describe function.\n",
    "3. If there is any duplicate value drop it.\n",
    "4. Use limit function for showcasing a limited number of\n",
    "records.\n",
    "5. If you find the column name is not suitable, change the\n",
    "column name.[optional]\n",
    "6. Select the subset of the columns.\n",
    "7. If there is any null value, fill it with any random value or drop\n",
    "it.\n",
    "8. Filter the data based on different columns or variables and\n",
    "do the best analysis.\n",
    "<br>For example: We can filter a data frame using multiple\n",
    "conditions using AND(&), OR(|) and NOT(~) conditions. For\n",
    "example, we may want to find out all the dif erent\n",
    "infection_case in Daegu Province with more than 10\n",
    "confirmed cases.</br>\n",
    "9. Sort the number of confirmed cases. Confirmed column is\n",
    "there in the dataset. Check with descending sort also.\n",
    "10. In case of any wrong data type, cast that data type from\n",
    "integer to string or string to integer.\n",
    "Use group by on top of province and city column and agg it\n",
    "with sum of confirmed cases. For example\n",
    "df.groupBy([\"province\",\"city\"]).agg(function.sum(\"co\n",
    "nfirmed\")\n",
    "11. For joins we will need one more file.you can use region file.\n",
    "User different different join methods.for example\n",
    "cases.join(regions, ['province','city'],how='left')\n",
    "You can do your best analysis.\n",
    "\n",
    "Step 5 - If you want, you can also use SQL with data frames. Let us try to\n",
    "run some SQL on the cases table.<br>\n",
    "For example:<br>\n",
    "cases.registerTempTable('cases_table')<br>\n",
    "newDF = sqlContext.sql('select * from cases_table where\n",
    "confirmed>100')<br>\n",
    "newDF.show()\n",
    "<br>\n",
    "<t>\n",
    "<br>\n",
    "Here is a example how you can use df for sql now you can perform\n",
    "various operations with GROUP BY, HAVING, AND ORDER BY\n",
    "\n",
    "Step 6 - Create Spark UDFs\n",
    "Create function casehighlow()<br>\n",
    "If case is less than 50 return low else return high<br>\n",
    "convert into a UDF Function and mention the return type of\n",
    "function.<br>\n",
    "Note: You can create as many as udf based on analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Create a producer with a python connector in confluent kafka and stream your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all needed packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import argparse\n",
    "from uuid import uuid4\n",
    "from six.moves import input\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.json_schema import JSONSerializer\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka essentials details\n",
    "\n",
    "# Variables\n",
    "API_KEY = 'AMTQUJ4OYGGMNYOJ'\n",
    "ENDPOINT_SCHEMA_URL  = 'https://psrc-8qyy0.eastus2.azure.confluent.cloud'\n",
    "API_SECRET_KEY = '4HTAJgLsRtyeQjCpKZItrYiCs7eduapoIzAkFApIZer4nzPQ2i53tPWe58TGuPY/'\n",
    "BOOTSTRAP_SERVER = 'pkc-n00kk.us-east-1.aws.confluent.cloud:9092'\n",
    "SECURITY_PROTOCOL = 'SASL_SSL'\n",
    "SSL_MACHENISM = 'PLAIN'\n",
    "SCHEMA_REGISTRY_API_KEY = 'BS3PHBXGYPRUDGFP'\n",
    "SCHEMA_REGISTRY_API_SECRET = 'gMwNVdkw1Cn5pFHmtFKrZam64E5HeDEo1dJu2hxCcUSSjaIkuscBUw6ucAlmjT2l'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Producer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sasl_conf():\n",
    "    # connection of producer to kakfa confluent  \n",
    "    sasl_conf = {'sasl.mechanism': SSL_MACHENISM,\n",
    "                 # Set to SASL_SSL to enable TLS support.\n",
    "                #  'security.protocol': 'SASL_PLAINTEXT'}\n",
    "                'bootstrap.servers':BOOTSTRAP_SERVER,\n",
    "                'security.protocol': SECURITY_PROTOCOL,\n",
    "                'sasl.username': API_KEY,\n",
    "                'sasl.password': API_SECRET_KEY\n",
    "                }\n",
    "    return sasl_conf\n",
    "\n",
    "\n",
    "def schema_config():\n",
    "    # schema registry authentication\n",
    "    return {'url':ENDPOINT_SCHEMA_URL,\n",
    "    'basic.auth.user.info':f\"{SCHEMA_REGISTRY_API_KEY}:{SCHEMA_REGISTRY_API_SECRET}\"\n",
    "    }\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    \"\"\"\n",
    "    Reports the success or failure of a message delivery.\n",
    "    Args:\n",
    "        err (KafkaError): The error that occurred on None on success.\n",
    "        msg (Message): The message that was produced or failed.\n",
    "    \"\"\"\n",
    "\n",
    "    if err is not None:\n",
    "        print(\"Delivery failed for User record {}: {}\".format(msg.key(), err))\n",
    "        return\n",
    "    print('User record {} successfully produced to {} [{}] at offset {}'.format(\n",
    "        msg.key(), msg.topic(), msg.partition(), msg.offset()))\n",
    "\n",
    "class Car: \n",
    "    # constructor  \n",
    "    def __init__(self,record:dict):\n",
    "        for k,v in record.items():\n",
    "            setattr(self,k,v)\n",
    "        \n",
    "        self.record=record\n",
    "   \n",
    "    @staticmethod\n",
    "    def dict_to_car(data:dict,ctx):\n",
    "        return Car(record=data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.record}\"\n",
    "\n",
    "def car_to_dict(car:Car, ctx):\n",
    "    \"\"\"\n",
    "    Returns a dict representation of a User instance for serialization.\n",
    "    Args:\n",
    "        user (User): User instance.\n",
    "        ctx (SerializationContext): Metadata pertaining to the serialization\n",
    "            operation.\n",
    "    Returns:\n",
    "        dict: Dict populated with user attributes to be serialized.\n",
    "    \"\"\"\n",
    "\n",
    "    # User._address must not be serialized; omit from dict\n",
    "    return car.record\n",
    "\n",
    "# read the data from csv file\n",
    "def get_car_instance(file_path, columns):\n",
    "    df=pd.read_csv(file_path)\n",
    "    df=df.iloc[:,:] \n",
    "    cars:List[Car]=[]\n",
    "    #df.replace(np.nan, '', regex=True)\n",
    "    nan_values = df.isna().any()\n",
    "    for col, val in nan_values.items():\n",
    "        #print(col, val)\n",
    "        #break\n",
    "        if val == True:\n",
    "            type_ = df[col].dtype.name\n",
    "            #print(type_)\n",
    "            if type_ == 'int64' or type_ == 'int32':\n",
    "                df[col].fillna(0, inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(\"*miss*\", inplace=True)\n",
    "    for data in df.values:\n",
    "        car=Car(dict(zip(columns,data)))\n",
    "        cars.append(car)\n",
    "        yield car\n",
    "\n",
    "def streamingToKafka(FILE_PATH, topic, schema_id, columns):\n",
    "    schema_registry_conf = schema_config()\n",
    "    schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "    schema_str = schema_registry_client.get_schema(schema_id).schema_str\n",
    "\n",
    "    string_serializer = StringSerializer('utf_8')\n",
    "    json_serializer = JSONSerializer(schema_str, schema_registry_client, car_to_dict)\n",
    "\n",
    "    producer = Producer(sasl_conf())\n",
    "\n",
    "    print(\"Producing user records to topic {}. ^C to exit.\".format(topic))\n",
    "    #while True:\n",
    "        # Serve on_delivery callbacks from previous calls to produce()\n",
    "    producer.poll(0.0)\n",
    "    try:\n",
    "        #for idx,car in enumerate(get_car_instance(FILE_PATH, columns)):\n",
    "        for car in get_car_instance(FILE_PATH, columns):\n",
    "            print(car)\n",
    "            producer.produce(topic=topic,\n",
    "                            key=string_serializer(str(uuid4()), car_to_dict),\n",
    "                            value=json_serializer(car, SerializationContext(topic, MessageField.VALUE)),\n",
    "                            on_delivery=delivery_report)\n",
    "            # if idx == 2:\n",
    "            #     break\n",
    "            #break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, discarding record...\")\n",
    "        pass\n",
    "\n",
    "    print(\"\\nFlushing records...\")\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added:  Case.csv\n",
      "Added:  PatientInfo.csv\n",
      "Added:  Policy.csv\n",
      "Added:  Region.csv\n",
      "Added:  SearchTrend.csv\n",
      "Added:  SeoulFloating.csv\n",
      "Added:  Time.csv\n",
      "Added:  TimeAge.csv\n",
      "Added:  TimeGender.csv\n",
      "Added:  TimeProvince.csv\n",
      "Added:  Weather.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name_list = []\n",
    "file_name = \"dataset.zip\"\n",
    "with ZipFile(file_name, 'r') as zipfile:\n",
    "    for zipInfo in zip.filelist:\n",
    "        print(\"Added: \", zipInfo.filename)\n",
    "        file_name_list.append(zipInfo.filename)\n",
    "    #zip.extractall()\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import build_table_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_basic_schema = {\n",
    "  \"$id\": \"http://example.com/myURI.schema.json\",\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"additionalProperties\": False,\n",
    "  \"description\": \"Sample schema to help you get started.\",\n",
    "  \"title\": \"SampleRecord\",\n",
    "  \"type\": \"object\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df = pd.read_csv('Case.csv')\n",
    "case_df_schema = build_table_schema(case_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'case_id'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'string', 'description': 'city'},\n",
       " {'type': 'boolean', 'description': 'group'},\n",
       " {'type': 'string', 'description': 'infection_case'},\n",
       " {'type': 'integer', 'description': 'confirmed'},\n",
       " {'type': 'string', 'description': 'latitude'},\n",
       " {'type': 'string', 'description': 'longitude'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in case_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "case_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df_final_schema = kafka_basic_schema.copy()\n",
    "case_df_final_schema['properties'] = {}\n",
    "for value in case_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    case_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$id': 'http://example.com/myURI.schema.json',\n",
       " '$schema': 'http://json-schema.org/draft-07/schema#',\n",
       " 'additionalProperties': False,\n",
       " 'description': 'Sample schema to help you get started.',\n",
       " 'title': 'SampleRecord',\n",
       " 'type': 'object',\n",
       " 'properties': {'case_id': {'type': 'integer'},\n",
       "  'province': {'type': 'string'},\n",
       "  'city': {'type': 'string'},\n",
       "  'group': {'type': 'boolean'},\n",
       "  'infection_case': {'type': 'string'},\n",
       "  'confirmed': {'type': 'integer'},\n",
       "  'latitude': {'type': 'string'},\n",
       "  'longitude': {'type': 'string'}}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_df_final_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_registry_conf = schema_config()\n",
    "# schema_registry_client = SchemaRegistryClient(schema_registry_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema.schema_str = json.dumps(case_df_final_schema)\n",
    "# Schema.schema_type = 'JSON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id = schema_registry_client.register_schema(\"case_schema\", Schema)\n",
    "# id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'Case.csv'\n",
    "topic = \"topic_case\"\n",
    "schema_id = 100005\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingToKafka(FILE_PATH, topic, schema_id, cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PatientInfo CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info_df = pd.read_csv('PatientInfo.csv')\n",
    "patient_info_df_schema = build_table_schema(patient_info_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'patient_id'},\n",
       " {'type': 'string', 'description': 'sex'},\n",
       " {'type': 'string', 'description': 'age'},\n",
       " {'type': 'string', 'description': 'country'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'string', 'description': 'city'},\n",
       " {'type': 'string', 'description': 'infection_case'},\n",
       " {'type': 'string', 'description': 'infected_by'},\n",
       " {'type': 'string', 'description': 'contact_number'},\n",
       " {'type': 'string', 'description': 'symptom_onset_date'},\n",
       " {'type': 'string', 'description': 'confirmed_date'},\n",
       " {'type': 'string', 'description': 'released_date'},\n",
       " {'type': 'string', 'description': 'deceased_date'},\n",
       " {'type': 'string', 'description': 'state'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in patient_info_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "patient_info_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info_df_final_schema = kafka_basic_schema.copy()\n",
    "patient_info_df_final_schema['properties'] = {}\n",
    "for value in patient_info_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    patient_info_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"patient_id\": {\"type\": \"integer\"}, \"sex\": {\"type\": \"string\"}, \"age\": {\"type\": \"string\"}, \"country\": {\"type\": \"string\"}, \"province\": {\"type\": \"string\"}, \"city\": {\"type\": \"string\"}, \"infection_case\": {\"type\": \"string\"}, \"infected_by\": {\"type\": \"string\"}, \"contact_number\": {\"type\": \"string\"}, \"symptom_onset_date\": {\"type\": \"string\"}, \"confirmed_date\": {\"type\": \"string\"}, \"released_date\": {\"type\": \"string\"}, \"deceased_date\": {\"type\": \"string\"}, \"state\": {\"type\": \"string\"}}}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(patient_info_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id            False\n",
       "sex                   False\n",
       "age                   False\n",
       "country               False\n",
       "province              False\n",
       "city                  False\n",
       "infection_case        False\n",
       "infected_by           False\n",
       "contact_number        False\n",
       "symptom_onset_date    False\n",
       "confirmed_date        False\n",
       "released_date         False\n",
       "deceased_date         False\n",
       "state                 False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_info_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'PatientInfo.csv'\n",
    "topic = \"topic_patientinfo\"\n",
    "schema_id = 100006\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info_df = pd.read_csv('PatientInfo.csv')\n",
    "patient_info_df_schema = build_table_schema(patient_info_df, index=False, version=False)['fields']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'integer', 'description': 'patient_id'},\n",
       " {'type': 'string', 'description': 'sex'},\n",
       " {'type': 'string', 'description': 'age'},\n",
       " {'type': 'string', 'description': 'country'},\n",
       " {'type': 'string', 'description': 'province'},\n",
       " {'type': 'string', 'description': 'city'},\n",
       " {'type': 'string', 'description': 'infection_case'},\n",
       " {'type': 'string', 'description': 'infected_by'},\n",
       " {'type': 'string', 'description': 'contact_number'},\n",
       " {'type': 'string', 'description': 'symptom_onset_date'},\n",
       " {'type': 'string', 'description': 'confirmed_date'},\n",
       " {'type': 'string', 'description': 'released_date'},\n",
       " {'type': 'string', 'description': 'deceased_date'},\n",
       " {'type': 'string', 'description': 'state'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#renaming the dictionary key from name to description\n",
    "# to match the kafka schema string\n",
    "for value in patient_info_df_schema:\n",
    "    value['description'] = value.pop('name')\n",
    "\n",
    "patient_info_df_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info_df_final_schema = kafka_basic_schema.copy()\n",
    "patient_info_df_final_schema['properties'] = {}\n",
    "for value in patient_info_df_schema:\n",
    "    name = value['description']\n",
    "    value.pop('description')\n",
    "    patient_info_df_final_schema['properties'][name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$id\": \"http://example.com/myURI.schema.json\", \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"additionalProperties\": false, \"description\": \"Sample schema to help you get started.\", \"title\": \"SampleRecord\", \"type\": \"object\", \"properties\": {\"patient_id\": {\"type\": \"integer\"}, \"sex\": {\"type\": \"string\"}, \"age\": {\"type\": \"string\"}, \"country\": {\"type\": \"string\"}, \"province\": {\"type\": \"string\"}, \"city\": {\"type\": \"string\"}, \"infection_case\": {\"type\": \"string\"}, \"infected_by\": {\"type\": \"string\"}, \"contact_number\": {\"type\": \"string\"}, \"symptom_onset_date\": {\"type\": \"string\"}, \"confirmed_date\": {\"type\": \"string\"}, \"released_date\": {\"type\": \"string\"}, \"deceased_date\": {\"type\": \"string\"}, \"state\": {\"type\": \"string\"}}}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(patient_info_df_final_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id            False\n",
       "sex                   False\n",
       "age                   False\n",
       "country               False\n",
       "province              False\n",
       "city                  False\n",
       "infection_case        False\n",
       "infected_by           False\n",
       "contact_number        False\n",
       "symptom_onset_date    False\n",
       "confirmed_date        False\n",
       "released_date         False\n",
       "deceased_date         False\n",
       "state                 False\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patient_info_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'PatientInfo.csv'\n",
    "topic = \"topic_patientinfo\"\n",
    "schema_id = 100006\n",
    "cols = list(pd.read_csv(FILE_PATH).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "streamingToKafka(FILE_PATH, topic, schema_id, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
